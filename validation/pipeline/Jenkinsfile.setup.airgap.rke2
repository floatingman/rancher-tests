#!/usr/bin/env groovy

/**
 * Ansible Airgap Setup Jenkinsfile
 * Based on Jenkinsfile.recurring but adapted for airgap RKE2 infrastructure setup
 *
 * This pipeline sets up airgap RKE2 infrastructure using Ansible and OpenTofu
 * with enhanced error handling and proper workspace management.
 *
 * The pipeline uses the ANSIBLE_VARIABLES parameter to provide the complete
 * group_vars/all.yml content for Ansible configuration.
 *
**/

// ========================================
// CONSTANTS AND CONFIGURATION
// ========================================
class PipelineConfig {
    static final String DEFAULT_HOSTNAME_PREFIX = 'airgap-ansible-jenkins'
    static final String DEFAULT_RKE2_VERSION = 'v1.28.8+rke2r1'
    static final String DEFAULT_RANCHER_VERSION = 'v2.9-head'
    static final String DEFAULT_RANCHER_TEST_REPO = 'https://github.com/rancher/tests.git'
    static final String DEFAULT_QA_INFRA_REPO = 'https://github.com/rancher/qa-infra-automation.git'
    static final String DEFAULT_S3_BUCKET = 'rancher-terraform-state'
    static final String DEFAULT_S3_REGION = 'us-east-1'
    static final String CONTAINER_NAME_PREFIX = 'rancher-ansible-airgap'
    static final String SHARED_VOLUME_PREFIX = 'validation-volume'
    static final String DOCKER_BUILD_CONTEXT = '.'
    static final String DOCKERFILE_PATH = 'tests/validation/Dockerfile.tofu.e2e'
    static final int TERRAFORM_TIMEOUT_MINUTES = 60
    static final int ANSIBLE_TIMEOUT_MINUTES = 90
    static final int VALIDATION_TIMEOUT_MINUTES = 30
    static final String LOG_PREFIX_INFO = '[INFO]'
    static final String LOG_PREFIX_ERROR = '[ERROR]'
    static final String LOG_PREFIX_WARNING = '[WARNING]'
}

// ========================================
// CONSOLIDATED ENVIRONMENT CONFIGURATION
// ========================================
def configureEnvironmentComplete() {
    logInfo('Configuring complete environment setup')
    
    // Step 1: Read and validate Ansible variables
    readAndValidateAnsibleVariables()
    
    // Step 2: Setup dynamic environment variables
    env.RKE2_VERSION = params.RKE2_VERSION
    env.RANCHER_VERSION = params.RANCHER_VERSION
    env.TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
    env.RANCHER_HOSTNAME = "${params.HOSTNAME_PREFIX ?: PipelineConfig.DEFAULT_HOSTNAME_PREFIX}.qa.rancher.space"
    
    logInfo("RKE2 Version: ${env.RKE2_VERSION}")
    logInfo("Rancher Version: ${env.RANCHER_VERSION}")
    logInfo("Rancher Hostname: ${env.RANCHER_HOSTNAME}")
    
    // Step 3: Configure credentials and generate files
    // NOTE: SSH keys must be created AFTER deleteDir() is called
    // This function is called AFTER workspace cleanup
    withCredentials(getCredentialsList()) {
        setupSSHKeysSecure()
        generateEnvironmentFileComplete()
        validateSensitiveDataHandling()
    }
    
    logInfo('Environment configuration completed successfully')
}

/**
 * Read and validate Ansible variables from parameter
 */
def readAndValidateAnsibleVariables() {
    if (!params.ANSIBLE_VARIABLES || !params.ANSIBLE_VARIABLES.trim()) {
        error('ANSIBLE_VARIABLES parameter is required but was not provided')
    }
    
    def ansibleVarsContent = params.ANSIBLE_VARIABLES.trim()
    logInfo("Ansible variables loaded: ${ansibleVarsContent.length()} bytes")
    
    env.ANSIBLE_VARIABLES = ansibleVarsContent
}

/**
 * Setup SSH keys with secure handling
 */
def setupSSHKeysSecure() {
    if (!env.AWS_SSH_PEM_KEY || !env.AWS_SSH_KEY_NAME) {
        logWarning('SSH key configuration skipped - credentials not available')
        return
    }
    
    logInfo('Setting up SSH keys')
    
    try {
        // Create SSH directory in workspace
        dir('./tests/.ssh') {
            sh 'mkdir -p . && chmod 700 .'
            
            def decodedKey = new String(env.AWS_SSH_PEM_KEY.decodeBase64())
            writeFile file: env.AWS_SSH_KEY_NAME, text: decodedKey
            
            sh "chmod 600 ${env.AWS_SSH_KEY_NAME}"
            sh "chown \$(whoami):\$(whoami) ${env.AWS_SSH_KEY_NAME} 2>/dev/null || true"
            
            // Validate key format
            def keyContent = sh(script: "head -1 ${env.AWS_SSH_KEY_NAME}", returnStdout: true).trim()
            if (!keyContent.startsWith('-----BEGIN')) {
                logWarning('SSH key format validation warning - unexpected format')
            }
            
            // Also generate public key for SSH operations
            sh """
                if [ -f "${env.AWS_SSH_KEY_NAME}" ]; then
                    ssh-keygen -f "${env.AWS_SSH_KEY_NAME}" -y > "${env.AWS_SSH_KEY_NAME}.pub" || echo "Failed to generate public key"
                    chmod 644 "${env.AWS_SSH_KEY_NAME}.pub" 2>/dev/null || true
                fi
            """
        }
        
        env.SSH_KEY_PATH = "./tests/.ssh/${env.AWS_SSH_KEY_NAME}"
        logInfo('SSH keys configured successfully')
        
    } catch (Exception e) {
        logError("SSH key setup failed: ${e.message}")
        cleanupSSHKeys()
        throw e
    }
}

/**
 * Generate complete environment file for container execution
 */
def generateEnvironmentFileComplete() {
    logInfo('Generating environment file')
    
    // Parse Terraform config for AWS variables
    def awsVars = extractAWSVariablesFromTerraformConfig()
    
    def envLines = [
        '# Environment Configuration - Credentials passed via withCredentials',
        "TF_WORKSPACE=${env.TF_WORKSPACE}",
        "BUILD_NUMBER=${env.BUILD_NUMBER}",
        "JOB_NAME=${env.JOB_NAME}",
        "TERRAFORM_TIMEOUT=${env.TERRAFORM_TIMEOUT}",
        "ANSIBLE_TIMEOUT=${env.ANSIBLE_TIMEOUT}",
        "QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH}",
        "TERRAFORM_VARS_FILENAME=${env.TERRAFORM_VARS_FILENAME}",
        "ANSIBLE_VARS_FILENAME=${env.ANSIBLE_VARS_FILENAME}",
        "RKE2_VERSION=${env.RKE2_VERSION}",
        "RANCHER_VERSION=${env.RANCHER_VERSION}",
        "HOSTNAME_PREFIX=${env.HOSTNAME_PREFIX}",
        "RANCHER_HOSTNAME=${env.RANCHER_HOSTNAME}",
        "PRIVATE_REGISTRY_URL=${env.PRIVATE_REGISTRY_URL}",
        "PRIVATE_REGISTRY_USERNAME=${env.PRIVATE_REGISTRY_USERNAME}",
        '',
        '# Ansible Configuration',
        "ANSIBLE_VARIABLES=${env.ANSIBLE_VARIABLES}",
        '',
        '# AWS Configuration',
        "AWS_REGION=${env.AWS_REGION}",
        "AWS_AMI=${awsVars['AWS_AMI'] ?: ''}",
        "AWS_HOSTNAME_PREFIX=${awsVars['AWS_HOSTNAME_PREFIX'] ?: env.HOSTNAME_PREFIX}",
        "AWS_SSH_USER=${awsVars['AWS_SSH_USER'] ?: 'ec2-user'}",
        "INSTANCE_TYPE=${awsVars['INSTANCE_TYPE'] ?: 't3.large'}",
        '',
        '# S3 Backend',
        "S3_BUCKET_NAME=${env.S3_BUCKET_NAME}",
        "S3_REGION=${env.S3_REGION}",
        "S3_KEY_PREFIX=${env.S3_KEY_PREFIX}",
        '',
        "AWS_SSH_KEY_NAME=${env.AWS_SSH_KEY_NAME ?: ''}",
        '# Sensitive credentials passed via withCredentials block'
    ]
    
    writeFile file: env.ENV_FILE, text: envLines.join('\n')
    logInfo("Environment file created: ${env.ENV_FILE}")
}

/**
 * Extract AWS variables from Terraform config parameter
 */
def extractAWSVariablesFromTerraformConfig() {
    def awsVars = [:]
    
    if (!env.TERRAFORM_CONFIG) {
        return awsVars
    }
    
    def config = env.TERRAFORM_CONFIG
    def pattern = ~/(\w+)\s*=\s*"([^"]*)"/
    
    config.eachMatch(pattern) { match ->
        def varName = match[1]
        def varValue = match[2]
        if (varName.startsWith('AWS_') || varName == 'INSTANCE_TYPE') {
            awsVars[varName] = varValue
        }
    }
    
    return awsVars
}

// ========================================
// PIPELINE DEFINITION
// ========================================

// Lazily-loadable Docker helper accessor (must be defined before pipeline execution)
dockerHelperInstance = null

def dockerHelper() {
    if (dockerHelperInstance == null) {
        // Try a few likely locations for the helper file. Jenkins may perform
        // additional checkouts (into subdirs like 'tests') which moves file
        // paths around during the run; be tolerant and try multiple candidates.
        def candidates = [
            'validation/pipeline/scripts/airgap/docker_helper.groovy',
            'tests/validation/pipeline/scripts/airgap/docker_helper.groovy'
        ]

        def helperPath = null
        for (p in candidates) {
            try {
                if (fileExists(p)) {
                    helperPath = p
                    break
                }
            } catch (ignored) {
                // fileExists may not be available outside node context; ignore and continue
            }
        }

        if (helperPath == null) {
            error("docker_helper.groovy not found in workspace at any of: ${candidates}")
        }

        def helperScript = load(helperPath)
        dockerHelperInstance = helperScript.init(this)
    }
    return dockerHelperInstance
}

// CI helpers loader (Phase 0 scaffold)
__ciHelpersRef = null

def ciHelpers() {
    if (__ciHelpersRef == null) {
        def candidates = [
            'validation/pipeline/ci/helpers.groovy',
            'tests/validation/pipeline/ci/helpers.groovy'
        ]
        for (p in candidates) {
            try {
                if (fileExists(p)) { __ciHelpersRef = load(p); break }
            } catch (ignored) {}
        }
    }
    return __ciHelpersRef
}

// Airgap steps loader (Phase 2)
__ciAirgapRef = null

def ciAirgap() {
    if (__ciAirgapRef == null) {
        def candidates = [
            'validation/pipeline/ci/airgap.groovy',
            'tests/validation/pipeline/ci/airgap.groovy'
        ]
        for (p in candidates) {
            try {
                if (fileExists(p)) { __ciAirgapRef = load(p); break }
            } catch (ignored) {}
        }
    }
    return __ciAirgapRef
}

pipeline {
    agent any

    // Global pipeline options
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timeout(time: 3, unit: 'HOURS')
        timestamps()
        ansiColor('xterm')
        skipStagesAfterUnstable()
        retry(1)
    }

    // Environment-specific parameters
    parameters {
        string(
            name: 'RKE2_VERSION',
            defaultValue: PipelineConfig.DEFAULT_RKE2_VERSION,
            description: 'RKE2 version to deploy (e.g., v1.28.8+rke2r1, v1.29.5+rke2r1, v1.30.2+rke2r1)'
        )
        string(
            name: 'RANCHER_VERSION',
            defaultValue: PipelineConfig.DEFAULT_RANCHER_VERSION,
            description: 'Rancher version to deploy (e.g., head, v2.10-head, v2.11.0, v2.9-head)'
        )
        string(
            name: 'RANCHER_TEST_REPO_URL',
            defaultValue: PipelineConfig.DEFAULT_RANCHER_TEST_REPO,
            description: 'URL of rancher/tests repository'
        )
        string(
            name: 'RANCHER_TEST_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of rancher/tests repository'
        )
        string(
            name: 'QA_INFRA_REPO_URL',
            defaultValue: PipelineConfig.DEFAULT_QA_INFRA_REPO,
            description: 'URL of qa-infra-automation repository'
        )
        string(
            name: 'QA_INFRA_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of qa-infra-automation repository'
        )
        string(
            name: 'PRIVATE_REGISTRY_URL',
            defaultValue: '',
            description: 'Private registry URL for airgap deployment'
        )
        string(
            name: 'PRIVATE_REGISTRY_USERNAME',
            defaultValue: 'default-user',
            description: 'Private registry username for airgap deployment'
        )
        password(
            name: 'PRIVATE_REGISTRY_PASSWORD',
            defaultValue: '',
            description: 'Private registry password for airgap deployment'
        )
        string(
            name: 'S3_BUCKET_NAME',
            defaultValue: PipelineConfig.DEFAULT_S3_BUCKET,
            description: 'S3 bucket name where Terraform state is stored'
        )
        string(
            name: 'S3_KEY_PREFIX',
            defaultValue: 'jenkins-airgap-rke2/',
            description: 'S3 key prefix for the Terraform state files'
        )
        string(
            name: 'S3_REGION',
            defaultValue: PipelineConfig.DEFAULT_S3_REGION,
            description: 'AWS region where the S3 bucket is located'
        )
        string(
            name: 'HOSTNAME_PREFIX',
            defaultValue: PipelineConfig.DEFAULT_HOSTNAME_PREFIX,
            description: 'Hostname prefix for *.qa.rancher.space and other AWS resources'
        )
        booleanParam(
            name: 'DESTROY_ON_FAILURE',
            defaultValue: true,
            description: 'Destroy infrastructure when Ansible playbooks fail (automatic cleanup)'
        )
        text(
            name: 'TERRAFORM_CONFIG',
            defaultValue: '',
            description: 'Terraform variables configuration for OpenTofu deployment'
        )
        text(
            name: 'ANSIBLE_VARIABLES',
            description: 'These config values are for the rancher instance use for the recurring runs.'
        )
        booleanParam(
            name: 'SKIP_YAML_VALIDATION',
            defaultValue: false,
            description: 'Skip strict YAML validation for templated group_vars payloads'
        )
        booleanParam(
            name: 'DEBUG',
            defaultValue: false,
            description: 'Enable debug output and verbose logging throughout the pipeline'
        )
    }

    // Global environment variables
    environment {
        // Repository configurations
        RANCHER_TEST_REPO_URL = "${params.RANCHER_TEST_REPO_URL ?: PipelineConfig.DEFAULT_RANCHER_TEST_REPO}"
        QA_INFRA_REPO = "${params.QA_INFRA_REPO_URL ?: PipelineConfig.DEFAULT_QA_INFRA_REPO}"

        // Private registry configurations
        PRIVATE_REGISTRY_URL = "${params.PRIVATE_REGISTRY_URL ?: ''}"
        PRIVATE_REGISTRY_USERNAME = "${params.PRIVATE_REGISTRY_USERNAME ?: 'default-user'}"
        PRIVATE_REGISTRY_PASSWORD = "${params.PRIVATE_REGISTRY_PASSWORD ?: ''}"

        // Path configurations
        ROOT_PATH = '/root/go/src/github.com/rancher/tests/'
        QA_INFRA_WORK_PATH = '/root/go/src/github.com/rancher/qa-infra-automation'

        // Cleanup configurations
        DESTROY_ON_FAILURE = "${params.DESTROY_ON_FAILURE}"

        // Skip strict YAML Validation
        SKIP_YAML_VALIDATION = "${params.SKIP_YAML_VALIDATION}"

        // Debug mode
        DEBUG = "${params.DEBUG}"

        // Computed values
        JOB_SHORT_NAME = "${getShortJobName()}"
        BUILD_CONTAINER_NAME = "${PipelineConfig.CONTAINER_NAME_PREFIX}-${env.JOB_SHORT_NAME}${env.BUILD_NUMBER}"
        IMAGE_NAME = "rancher-ansible-airgap-setup-${env.JOB_SHORT_NAME}${env.BUILD_NUMBER}"
        VALIDATION_VOLUME = "${PipelineConfig.SHARED_VOLUME_PREFIX}-${env.JOB_SHORT_NAME}${env.BUILD_NUMBER}"

        // Configuration files
        ANSIBLE_VARS_FILENAME = 'vars.yaml'
        TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
        TERRAFORM_BACKEND_CONFIG_FILENAME = 'backend.tf'
        ENV_FILE = '.env'

        // Terraform workspace
        TF_WORKSPACE = "jenkins_airgap_ansible_workspace_${env.BUILD_NUMBER}"

        // Timeouts (in minutes)
        TERRAFORM_TIMEOUT = "${PipelineConfig.TERRAFORM_TIMEOUT_MINUTES}"
        ANSIBLE_TIMEOUT = "${PipelineConfig.ANSIBLE_TIMEOUT_MINUTES}"
        VALIDATION_TIMEOUT = "${PipelineConfig.VALIDATION_TIMEOUT_MINUTES}"

        // Backend configuration (S3 backend parameters)
        S3_BUCKET_NAME = "${params.S3_BUCKET_NAME ?: PipelineConfig.DEFAULT_S3_BUCKET}"
        S3_REGION = "${params.S3_REGION ?: PipelineConfig.DEFAULT_S3_REGION}"
        AWS_REGION = "${params.S3_REGION ?: PipelineConfig.DEFAULT_S3_REGION}"
        S3_KEY_PREFIX = "${params.S3_KEY_PREFIX ?: 'jenkins-airgap-rke2'}"

        // Hostname prefix
        HOSTNAME_PREFIX = "${params.HOSTNAME_PREFIX ?: PipelineConfig.DEFAULT_HOSTNAME_PREFIX}"
        RANCHER_HOSTNAME = "${(params.HOSTNAME_PREFIX ?: PipelineConfig.DEFAULT_HOSTNAME_PREFIX)}.qa.rancher.space"

        // Configuration content from parameters
        TERRAFORM_CONFIG = "${params.TERRAFORM_CONFIG ?: ''}"
        ANSIBLE_VARIABLES = "${params.ANSIBLE_VARIABLES ?: ''}"
    }

    stages {
        stage('Initialize Pipeline') {
            steps {
                script {
                    def s = ciAirgap()
                    s.setupEnv(this)
                }
            }
        }

        stage('Checkout Repositories') {
            steps {
                script {
                    def s = ciAirgap()
                    s.checkoutRepositories(this)
                }
            }
        }

        stage('Configure Environment') {
            steps {
                script {
                    def s = ciAirgap()
                    s.configureEnv(this)
                }
            }
        }

        stage('Prepare Infrastructure') {
            steps {
                script {
                    def s = ciAirgap()
                    s.prepareInfra(this)
                }
            }
        }

        stage('Deploy Infrastructure') {
            steps {
                script {
                    def s = ciAirgap()
                    s.deployInfrastructure(this)
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('deployment') }
                }
            }
        }

        stage('Prepare Ansible Environment') {
            steps {
                script {
                    def s = ciAirgap()
                    s.prepareAnsibleEnv(this)
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('ansible_prep') }
                }
            }
        }

        stage('Deploy RKE2 with Ansible') {
            steps {
                script {
                    def s = ciAirgap()
                    s.deployRKE2(this)
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('rke2') }
                }
            }
        }

        stage('Deploy Rancher with Ansible') {
            steps {
                script {
                    def s = ciAirgap()
                    s.deployRancher(this)
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('rancher') }
                }
            }
        }
    }

    post {
        always {
            script {
                logInfo('Starting post-build cleanup')

                // Archive important artifacts (excluding sensitive tfstate and tfvars files)
                archiveBuildArtifacts([
                    'kubeconfig.yaml',
                    'infrastructure-outputs.json',
                    'ansible-inventory.yml',
                    'ansible-logs.txt',
                    'deployment-summary.json'
                ])

                // Always cleanup containers and volumes
                try {
                    node {
                        cleanupContainersAndVolumes()
                    }
                } catch (Exception e) {
                    logError("Node context not available for cleanup: ${e.message}")
                    try {
                        cleanupContainersAndVolumes()
                    } catch (Exception cleanupException) {
                        logError("Cleanup failed: ${cleanupException.message}")
                    }
                }
            }
        }

        success {
            script {
                logInfo('Pipeline completed successfully')
            }
        }

        failure {
            script {
                logError('Pipeline failed')

                // Use consolidated cleanup script for failure
                if (env.DESTROY_ON_FAILURE.toBoolean()) {
                    logInfo('DESTROY_ON_FAILURE is true - using consolidated cleanup script for pipeline failure')
                    def cleanupScript = '''
#!/bin/bash
set -e
# Source the consolidated cleanup script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/airgap_cleanup.sh
# Execute cleanup for deployment failure
perform_cleanup "deployment_failure" "${TF_WORKSPACE}" "true"
'''
                    def cleanupEnvVars = [
                        'DESTROY_ON_FAILURE': env.DESTROY_ON_FAILURE,
                        'QA_INFRA_WORK_PATH': env.QA_INFRA_WORK_PATH,
                        'TF_WORKSPACE': env.TF_WORKSPACE,
                        'TERRAFORM_BACKEND_CONFIG_FILENAME': env.TERRAFORM_BACKEND_CONFIG_FILENAME,
                        'TERRAFORM_VARS_FILENAME': env.TERRAFORM_VARS_FILENAME
                    ]
                    dockerHelper().executeScriptInContainer(cleanupScript, cleanupEnvVars)
                }
            }
        }

        aborted {
            script {
                logWarning('Pipeline was aborted')

                // Use consolidated cleanup script for timeout/abort
                if (env.DESTROY_ON_FAILURE.toBoolean()) {
                    logInfo('DESTROY_ON_FAILURE is true - using consolidated cleanup script for pipeline timeout')
                    def cleanupScript = '''
#!/bin/bash
set -e
# Source the consolidated cleanup script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/airgap_cleanup.sh
# Execute cleanup for timeout
perform_cleanup "timeout" "${TF_WORKSPACE}" "true"
'''
                    def cleanupEnvVars = [
                        'DESTROY_ON_FAILURE': env.DESTROY_ON_FAILURE,
                        'QA_INFRA_WORK_PATH': env.QA_INFRA_WORK_PATH,
                        'TF_WORKSPACE': env.TF_WORKSPACE,
                        'TERRAFORM_BACKEND_CONFIG_FILENAME': env.TERRAFORM_BACKEND_CONFIG_FILENAME,
                        'TERRAFORM_VARS_FILENAME': env.TERRAFORM_VARS_FILENAME
                    ]
                    dockerHelper().executeScriptInContainer(cleanupScript, cleanupEnvVars)
                }
            }
        }

        unstable {
            script {
                logWarning('Pipeline completed with warnings')
            }
        }
    }
}

// ========================================
// LOGGING UTILITY FUNCTIONS
// ========================================


def logInfo(msg) {
    echo "${PipelineConfig.LOG_PREFIX_INFO} ${getTimestamp()} ${msg}"
}

def logError(msg) {
    echo "${PipelineConfig.LOG_PREFIX_ERROR} ${getTimestamp()} ${msg}"
}

def logWarning(msg) {
    echo "${PipelineConfig.LOG_PREFIX_WARNING} ${getTimestamp()} ${msg}"
}


static def getTimestamp() {
    return new Date().format('yyyy-MM-dd HH:mm:ss')
}

// ========================================
// PARAMETER VALIDATION FUNCTIONS
// ========================================

def validateParameters() {
    logInfo('Validating pipeline parameters using external script')

    def validationScript = '''
#!/bin/bash
set -e
# Source the external validation helper script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/validation_helpers.sh
# Run parameter validation
validate_pipeline_parameters
'''

    def validationEnvVars = [
            'RKE2_VERSION': params.RKE2_VERSION,
            'RANCHER_VERSION': params.RANCHER_VERSION,
            'RANCHER_TEST_REPO_URL': params.RANCHER_TEST_REPO_URL,
            'QA_INFRA_REPO_URL': params.QA_INFRA_REPO_URL,
    ]

    try {
        dockerHelper().executeScriptInContainer(validationScript, validationEnvVars)
        logInfo('All parameters validated successfully')
    } catch (Exception e) {
        def errorMsg = "Parameter validation failed: ${e.message}"
        logError(errorMsg)
        error(errorMsg)
    }
}

def validateRequiredVariables(List<String> requiredVars) {
    logInfo('Validating required environment variables')

    def validationScript = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/validation_helpers.sh
validate_required_variables ''' + requiredVars.join(' ')

    def envVars = [:]
    requiredVars.each { varName ->
        envVars[varName] = env."${varName}" ?: ''
    }

    try {
        dockerHelper().executeScriptInContainer(validationScript, envVars)
        logInfo('All required variables validated successfully')
    } catch (Exception e) {
        def errorMsg = "Required variables validation failed: ${e.message}"
        logError(errorMsg)
        error(errorMsg)
    }
}


def validateSensitiveDataHandling() {
    logInfo('Validating sensitive data handling using external script')

    def validationScript = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/validation_helpers.sh
validate_sensitive_data_handling "${ENV_FILE}"
'''

    def validationEnvVars = [
            'ENV_FILE': env.ENV_FILE,
            'SSH_KEY_PATH': env.SSH_KEY_PATH ?: ''
    ]

    try {
        dockerHelper().executeScriptInContainer(validationScript, validationEnvVars)
        logInfo('Sensitive data handling validated successfully')
    } catch (Exception e) {
        // Log warning but don't fail the build
        logWarning("Sensitive data validation issues detected: ${e.message}")
        logWarning('Review security measures and address any concerns')
    }
}

// ========================================
// ENVIRONMENT SETUP FUNCTIONS
// ========================================

def getCredentialsList() {
    return [
        string(credentialsId: 'AWS_ACCESS_KEY_ID', variable: 'AWS_ACCESS_KEY_ID'),
        string(credentialsId: 'AWS_SECRET_ACCESS_KEY', variable: 'AWS_SECRET_ACCESS_KEY'),
        string(credentialsId: 'AWS_SSH_PEM_KEY', variable: 'AWS_SSH_PEM_KEY'),
        string(credentialsId: 'AWS_SSH_KEY_NAME', variable: 'AWS_SSH_KEY_NAME'),
    ]
}

def getShortJobName() {
    def jobName = "${env.JOB_NAME}"
    if (jobName.contains('/')) {
        def lastSlashIndex = jobName.lastIndexOf('/')
        return jobName.substring(lastSlashIndex + 1)
    }
    return jobName
}

// ========================================
// CONFIGURATION GENERATION FUNCTIONS
// ========================================

def cleanupSSHKeys() {
    logInfo('Cleaning up SSH keys securely')

    try {
        // Get credentials for key name
        withCredentials([
            string(credentialsId: 'AWS_SSH_KEY_NAME', variable: 'AWS_SSH_KEY_NAME')
        ]) {
            def awsSshKeyName = env.AWS_SSH_KEY_NAME

            if (awsSshKeyName) {
                def keyPath = "./tests/.ssh/${awsSshKeyName}"

                if (fileExists(keyPath)) {
                    // Securely shred the key file if shred is available
                    try {
                        sh "shred -vfz -n 3 ${keyPath} 2>/dev/null || rm -f ${keyPath}"
                        logInfo("SSH key securely shredded: ${keyPath}")
                    } catch (Exception ignored) {
                        // Fallback to secure delete
                        sh "rm -f ${keyPath}"
                        logWarning("SSH key deleted (shred unavailable): ${keyPath}")
                    }
                }

                // Clean up any temporary SSH files
                sh 'rm -f ./tests/.ssh/known_hosts ./tests/.ssh/config 2>/dev/null || true'

                // Ensure SSH directory is secure
                if (fileExists('./tests/.ssh')) {
                    sh 'chmod 700 ./tests/.ssh 2>/dev/null || true'
                }
            }
        }
    } catch (Exception e) {
        logWarning("SSH key cleanup encountered issues: ${e.message}")
    // Continue with cleanup even if some steps fail
    }

    logInfo('SSH key cleanup completed')
}


// ========================================
// DOCKER MANAGEMENT FUNCTIONS
// ========================================

def buildDockerImage() {
    logInfo("Building Docker image: ${env.IMAGE_NAME}")

    dir(PipelineConfig.DOCKER_BUILD_CONTEXT) {
        // Run configure script silently
        sh './tests/validation/configure.sh > /dev/null 2>&1'

        // Build Docker image with minimal output
        sh """
            docker build . \\
                -f ${PipelineConfig.DOCKERFILE_PATH} \\
                -t ${env.IMAGE_NAME} \\
                --build-arg BUILD_DATE=\$(date -u +'%Y-%m-%dT%H:%M:%SZ') \\
                --build-arg VCS_REF=\$(git rev-parse --short HEAD) \\
                --label "pipeline.build.number=${env.BUILD_NUMBER}" \\
                --label "pipeline.job.name=${env.JOB_NAME}" \\
                --quiet
        """
    }

    logInfo('Docker image built successfully')
}

def createSharedVolume() {
    logInfo("Creating shared volume: ${env.VALIDATION_VOLUME}")
    sh "docker volume create --name ${env.VALIDATION_VOLUME}"
}

def ensureSSHKeysInContainer() {
    logInfo('Ensuring SSH keys are available in container')

    def sshKeyName = env.AWS_SSH_KEY_NAME
    def sshDir = "./tests/.ssh"
    def keyPath = sshKeyName ? "${sshDir}/${sshKeyName}" : null

    if (!sshKeyName || !fileExists(keyPath)) {
        if (!sshKeyName) {
            logWarning('AWS_SSH_KEY_NAME not set, attempting to provide keys')
        } else {
            logWarning("SSH key not found at: ${keyPath}")
        }

        // Try to recreate them from credentials if available
        def recreated = false
        try {
            withCredentials(getCredentialsList()) {
                try {
                    setupSSHKeysSecure()
                    recreated = true
                    logInfo('SSH keys recreated from credentials')
                } catch (Exception e) {
                    logWarning("Failed to recreate SSH keys from credentials: ${e.message}")
                }
            }
        } catch (ignored) {
            // withCredentials may fail outside node context; ignore and proceed to fallback
        }

        // Re-evaluate key path after attempted recreation
        if (recreated && sshKeyName) {
            keyPath = "${sshDir}/${sshKeyName}"
        }

        // If still missing, generate an ephemeral SSH keypair so containers have a usable key
        if (!keyPath || !fileExists(keyPath)) {
            logWarning('No SSH key present; generating ephemeral SSH keypair for this run')
            dir(sshDir) {
                sh """
                    mkdir -p . && chmod 700 .
                    set -e
                    # Generate ephemeral key without passphrase; overwrite if exists
                    ssh-keygen -t rsa -b 2048 -N "" -f ephemeral_jenkins_key -q || true
                    chmod 600 ephemeral_jenkins_key || true
                    chmod 644 ephemeral_jenkins_key.pub || true
                """
                // Use ephemeral key for subsequent copy
                keyPath = "${sshDir}/ephemeral_jenkins_key"
                sshKeyName = 'ephemeral_jenkins_key'
                logInfo("Ephemeral SSH key generated: ${keyPath}")
            }
        }
    }

    if (!keyPath || !fileExists(keyPath)) {
        logWarning('Still no SSH key available after attempts; continuing without copying keys')
        return
    }

    try {
        // Copy SSH keys into container volume.
        // Use a safe shell check to avoid 'cp' failing when /source is empty.
        sh """
            docker run --rm \\
                -v \${env.VALIDATION_VOLUME}:/target \\
                -v \$(pwd)/\${sshDir}:/source:ro \\
                alpine:latest \\
                sh -c '
                    if [ -d /source ] && [ -n "$(ls -A /source 2>/dev/null)" ]; then
                        mkdir -p /target/.ssh
                        chmod 700 /target/.ssh
                        cp /source/* /target/.ssh/ || { echo "Failed to copy keys"; exit 1; }
                        chmod 600 /target/.ssh/* || true
                        echo "SSH keys copied successfully"
                    else
                        echo "No SSH keys found in source directory; skipping copy"
                    fi
                '
        """
        logInfo('SSH keys successfully copied to container volume (or skipped if none present)')
    } catch (Exception e) {
        logError("Failed to copy SSH keys to container: ${e.message}")
        throw e
    }
}

def cleanupContainersAndVolumes() {
    // Prefer library cleanup implementation when available
    try {
        def s = ciAirgap()
        if (s && s.metaClass.respondsTo(s, 'cleanupContainersAndVolumes')) {
            s.cleanupContainersAndVolumes(this)
            return
        }
    } catch (ignored) {}
 
    logInfo('Cleaning up Docker containers and volumes (fallback)')
    try {
        if (env.NODE_NAME) {
            sh """
                docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker stop || true
                docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker rm -v || true
                docker rmi -f ${env.IMAGE_NAME} || true
                docker volume rm -f ${env.VALIDATION_VOLUME} || true
                docker system prune -f || true
            """
        } else {
            logWarning('No node context available for Docker cleanup')
        }
    } catch (Exception e) {
        logError("Docker cleanup failed: ${e.message}")
    }
 
    try {
        cleanupSSHKeys()
    } catch (ignored) {}
 
    try {
        if (fileExists(env.ENV_FILE)) {
            sh "shred -vfz -n 3 ${env.ENV_FILE} 2>/dev/null || rm -f ${env.ENV_FILE}"
            logInfo('Environment file securely shredded')
        }
    } catch (ignored) {}
}

// ========================================
// UNIFIED ARTIFACT MANAGEMENT
// ========================================

/**
 * Centralized artifact configuration map
 * Defines all artifact types and their associated files
 */
@NonCPS
static def getArtifactDefinitions() {
    return [
        'infrastructure': [
            'infrastructure-outputs.json',
            'ansible-inventory.yml',
            '*.tfvars'
        ],
        'ansible_prep': [
            'group_vars.tar.gz',
            'group_vars/all.yml',
            'ansible-preparation-report.txt'
        ],
        'rke2_deployment': [
            'kubeconfig.yaml',
            'rke2_deployment_report.txt',
            'rke2_deployment.log',
            'kubectl-setup-logs.txt'
        ],
        'rancher_deployment': [
            'rancher-deployment-logs.txt',
            'rancher-validation-logs.txt',
            'deployment-summary.json'
        ],
        'failure_common': [
            '*.log',
            'error-*.txt',
            'ansible-debug-info.txt'
        ],
        'failure_infrastructure': [
            'tfplan-backup',
            'infrastructure-outputs.json'
        ],
        'failure_ansible': [
            'ansible-inventory.yml',
            'ansible-error-logs.txt',
            'ssh-setup-error-logs.txt'
        ],
        'failure_rke2': [
            'rke2-deployment-error-logs.txt',
            'kubectl-setup-error-logs.txt'
        ],
        'failure_rancher': [
            'rancher-deployment-error-logs.txt',
            'rancher-validation-logs.txt',
            'rancher-debug-info.txt'
        ],
        'success_complete': [
            'kubeconfig.yaml',
            'infrastructure-outputs.json',
            'ansible-inventory.yml',
            'deployment-summary.json'
        ]
    ]
}

/**
 * Archive artifacts based on type
 */
def archiveArtifactsByType(String artifactType, List additionalFiles = []) {
    def artifactDefs = getArtifactDefinitions()
    
    if (!artifactDefs.containsKey(artifactType)) {
        logWarning("Unknown artifact type: ${artifactType}, using failure_common")
        artifactType = 'failure_common'
    }
    
    def artifactList = artifactDefs[artifactType] + additionalFiles
    
    logInfo("Archiving ${artifactType} artifacts")
    try {
        archiveArtifacts(
            artifacts: artifactList.join(','),
            allowEmptyArchive: true,
            fingerprint: true,
            onlyIfSuccessful: false
        )
    } catch (Exception e) {
        logError("Failed to archive ${artifactType} artifacts: ${e.message}")
    }
}

/**
 * Archive failure artifacts - combines common + specific
 */
def archiveFailureArtifactsByType(String failureType) {
    def typeMap = [
        'deployment': 'failure_infrastructure',
        'ansible_prep': 'failure_ansible',
        'rke2': 'failure_rke2',
        'rancher': 'failure_rancher',
        'timeout': 'failure_infrastructure',
        'aborted': 'failure_common'
    ]
    
    def specificType = typeMap[failureType] ?: 'failure_common'
    archiveArtifactsByType(specificType)
    
    if (specificType != 'failure_common') {
        archiveArtifactsByType('failure_common')
    }
}

def archiveBuildArtifacts(List artifacts) {
    try {
        archiveArtifacts(
            artifacts: artifacts.join(','),
            allowEmptyArchive: true
        )
    } catch (Exception e) {
        logError("Failed to archive artifacts: ${e.message}")
    }
}

/**
 * Unified cleanup handler for all failure scenarios
 * Replaces repetitive cleanup code in multiple post blocks
 *
 * @param failureType Type of failure: 'deployment', 'ansible_prep', 'rke2', 'rancher', 'timeout', 'aborted'
 */
def handleFailureCleanup(String failureType) {
    logError("${failureType} failure detected - initiating cleanup")

    try {
        // Extract artifacts before cleanup
        extractArtifactsFromDockerVolume()

        // Archive failure-specific artifacts
        archiveFailureArtifactsByType(failureType)

        // Execute infrastructure cleanup if enabled
        if (env.DESTROY_ON_FAILURE?.toBoolean()) {
            logInfo('DESTROY_ON_FAILURE is true - executing consolidated cleanup')
            executeInfrastructureCleanup(failureType)
        } else {
            logWarning('DESTROY_ON_FAILURE is false - manual cleanup required')
            logWarning("Please clean up resources in workspace: ${env.TF_WORKSPACE}")
        }

        // Always cleanup containers and volumes
        cleanupContainersAndVolumes()

    } catch (Exception cleanupException) {
        logError("Cleanup failed: ${cleanupException.message}")
        // Attempt container cleanup even if infrastructure cleanup fails
        try {
            cleanupContainersAndVolumes()
        } catch (Exception containerException) {
            logError("Container cleanup also failed: ${containerException.message}")
        }
    }
}

/**
 * Execute infrastructure cleanup using consolidated script
 *
 * @param cleanupReason Reason for cleanup: 'deployment_failure', 'timeout', 'aborted'
 */
def executeInfrastructureCleanup(String failureType) {
    // Delegate to library implementation when available
    try {
        def s = ciAirgap()
        if (s && s.metaClass.respondsTo(s, 'executeInfrastructureCleanup')) {
            s.executeInfrastructureCleanup(this, failureType)
            return
        }
    } catch (ignored) {}
 
    def cleanupReason = failureType in ['timeout', 'aborted'] ? failureType : 'deployment_failure'
    def cleanupScript = '''
#!/bin/bash
set -e
# Source the consolidated cleanup script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/airgap_cleanup.sh
# Execute cleanup
perform_cleanup "${CLEANUP_REASON}" "${TF_WORKSPACE}" "true"
'''
    def cleanupEnvVars = [
        'CLEANUP_REASON': cleanupReason,
        'DESTROY_ON_FAILURE': env.DESTROY_ON_FAILURE,
        'QA_INFRA_WORK_PATH': env.QA_INFRA_WORK_PATH,
        'TF_WORKSPACE': env.TF_WORKSPACE,
        'TERRAFORM_BACKEND_CONFIG_FILENAME': env.TERRAFORM_BACKEND_CONFIG_FILENAME,
        'TERRAFORM_VARS_FILENAME': env.TERRAFORM_VARS_FILENAME
    ]
    try {
        dockerHelper().executeScriptInContainer(cleanupScript, cleanupEnvVars)
        logInfo("Infrastructure cleanup completed for ${failureType} (fallback)")
    } catch (Exception e) {
        logError("Infrastructure cleanup failed: ${e.message}")
        throw e
    }
}

// ========================================
// ARTIFACT EXTRACTION FUNCTIONS
// ========================================

def extractArtifactsFromDockerVolume() {
    logInfo('Extracting artifacts from Docker shared volume to Jenkins workspace')

    try {
        // Create a temporary container to copy files from the shared volume
        def timestamp = System.currentTimeMillis()
        def extractorContainerName = "${env.BUILD_CONTAINER_NAME}-extractor-${timestamp}"

        sh """
            docker run --rm \\
                -v ${env.VALIDATION_VOLUME}:/source \\
                -v \$(pwd):/dest \\
                --name ${extractorContainerName} \\
                -e TERRAFORM_VARS_FILENAME=${env.TERRAFORM_VARS_FILENAME} \\
                alpine:latest \\
                sh -c '
                    # Copy infrastructure outputs if exists
                    [ -f /source/infrastructure-outputs.json ] && cp /source/infrastructure-outputs.json /dest/ || true

                    # Copy ansible inventory if exists (generated by Terraform)
                    if [ -f /source/ansible/rke2/airgap/inventory.yml ]; then
                        cp /source/ansible/rke2/airgap/inventory.yml /dest/ansible-inventory.yml
                    elif [ -f /source/ansible-inventory.yml ]; then
                        cp /source/ansible-inventory.yml /dest/
                    fi

                    # Copy terraform vars file if exists
                    [ -f "/source/${TERRAFORM_VARS_FILENAME}" ] && cp "/source/${TERRAFORM_VARS_FILENAME}" /dest/ || true

                    # Copy terraform state files if they exist
                    [ -f /source/terraform.tfstate ] && cp /source/terraform.tfstate /dest/ || true
                    [ -f /source/terraform-state-primary.tfstate ] && cp /source/terraform-state-primary.tfstate /dest/ || true

                    # Copy any backup state files
                    for backup_file in /source/terraform-state-backup-*.tfstate /source/tfstate-backup-*.tfstate; do
                        [ -f "\$backup_file" ] && cp "\$backup_file" /dest/ || true
                    done

                    # Copy kubeconfig if it exists
                    if [ -f /source/kubeconfig.yaml ]; then
                        cp /source/kubeconfig.yaml /dest/
                    elif [ -f /source/group_vars/kubeconfig.yaml ]; then
                        cp /source/group_vars/kubeconfig.yaml /dest/
                    fi

                    # Copy rendered group_vars if they exist
                    if [ -f /source/group_vars/all.yml ]; then
                        mkdir -p /dest/group_vars
                        cp /source/group_vars/all.yml /dest/group_vars/all.yml
                    fi
                '
        """

        generateDeploymentSummary()
        logInfo('Artifact extraction completed successfully')
    } catch (Exception e) {
        logError("Artifact extraction failed: ${e.message}")
        logWarning('Build will continue, but some artifacts may not be available for archival')
    }
}

def generateDeploymentSummary() {
    logInfo('Generating deployment summary')

    try {
        def timestamp = new Date().format('yyyy-MM-dd HH:mm:ss')
        def summary = [
            deployment_info: [
                timestamp: timestamp,
                build_number: env.BUILD_NUMBER,
                job_name: env.JOB_NAME,
                workspace: env.TF_WORKSPACE,
                rke2_version: env.RKE2_VERSION,
                rancher_version: env.RANCHER_VERSION,
                rancher_hostname: env.RANCHER_HOSTNAME
            ],
            infrastructure: [
                terraform_vars_file: env.TERRAFORM_VARS_FILENAME,
                s3_bucket: env.S3_BUCKET_NAME,
                s3_region: env.S3_REGION,
                hostname_prefix: env.HOSTNAME_PREFIX
            ],
            artifacts_generated: []
        ]

        // Check which artifacts were successfully generated
        def artifactFiles = [
            'infrastructure-outputs.json',
            'ansible-inventory.yml',
            env.TERRAFORM_VARS_FILENAME,
            'terraform.tfstate'
        ]

        artifactFiles.each { fileName ->
            if (fileExists(fileName)) {
                summary.artifacts_generated.add(fileName)
            }
        }

        def summaryJson = groovy.json.JsonOutput.toJson(summary)
        writeFile file: 'deployment-summary.json', text: groovy.json.JsonOutput.prettyPrint(summaryJson)

        logInfo('Deployment summary generated successfully')
    } catch (Exception e) {
        logError("Failed to generate deployment summary: ${e.message}")
        logError("Exception details: ${e}")
    }
}

// ========================================
// INFRASTRUCTURE DEPLOYMENT FUNCTIONS
// ========================================

def generateTofuConfiguration() {
    // Prefer library implementation when available
    try {
        def s = ciAirgap()
        if (s && s.metaClass.respondsTo(s, 'generateTofuConfiguration')) {
            s.generateTofuConfiguration(this)
            return
        }
    } catch (ignored) {}
 
    logInfo('Generating Terraform configuration (fallback)')
    if (!env.TERRAFORM_CONFIG) { error('TERRAFORM_CONFIG environment variable is not set') }
    if (!env.S3_BUCKET_NAME) { error('S3_BUCKET_NAME environment variable is not set') }
    if (!env.S3_REGION) { error('S3_REGION environment variable is not set') }
    if (!env.S3_KEY_PREFIX) { error('S3_KEY_PREFIX environment variable is not set') }
    sh 'mkdir -p qa-infra-automation/tofu/aws/modules/airgap'
    def terraformConfig = env.TERRAFORM_CONFIG
    terraformConfig = terraformConfig.replace('${AWS_SECRET_ACCESS_KEY}', env.AWS_SECRET_ACCESS_KEY ?: '')
    terraformConfig = terraformConfig.replace('${AWS_ACCESS_KEY_ID}', env.AWS_ACCESS_KEY_ID ?: '')
    terraformConfig = terraformConfig.replace('${HOSTNAME_PREFIX}', env.HOSTNAME_PREFIX ?: '')
    dir('./qa-infra-automation') {
        dir('./tofu/aws/modules/airgap') {
            writeFile file: env.TERRAFORM_VARS_FILENAME, text: terraformConfig
            logInfo("Terraform configuration written to: ${env.TERRAFORM_VARS_FILENAME}")
            def backendConfig = """
terraform {
  backend "s3" {
    bucket = "${env.S3_BUCKET_NAME}"
    key    = "${env.S3_KEY_PREFIX}"
    region = "${env.S3_REGION}"
  }
}
"""
            writeFile file: env.TERRAFORM_BACKEND_CONFIG_FILENAME, text: backendConfig
            logInfo("S3 backend configuration written to: ${env.TERRAFORM_BACKEND_CONFIG_FILENAME}")
        }
    }
}