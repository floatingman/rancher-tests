#!/usr/bin/env groovy

/**
 * Infrastructure Destruction Jenkinsfile for Airgap RKE2
 *
 * This pipeline is designed to safely destroy infrastructure created by
 * the main airgap RKE2 deployment pipeline. It retrieves Terraform state
 * from S3 backend and performs controlled destruction.
 */

pipeline {
    agent any

    // Global pipeline options
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timeout(time: 2, unit: 'HOURS')
        timestamps()
        ansiColor('xterm')
        skipStagesAfterUnstable()
    }

    parameters {
        string(
            name: 'TARGET_WORKSPACE',
            defaultValue: '',
            description: 'Terraform workspace to destroy (e.g., jenkins_airgap_ansible_workspace_123)'
        )
        string(
            name: 'RANCHER_TEST_REPO_URL',
            defaultValue: 'https://github.com/rancher/tests',
            description: 'URL of rancher/tests repository'
        )
        string(
            name: 'RANCHER_TEST_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of rancher/tests repository'
        )
        string(
            name: 'QA_INFRA_REPO_URL',
            defaultValue: 'https://github.com/rancher/qa-infra-automation',
            description: 'URL of qa-infra-automation repository'
        )
        string(
            name: 'QA_INFRA_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of qa-infra-automation repository'
        )
        string(
            name: 'S3_BUCKET_NAME',
            defaultValue: 'jenkins-terraform-state-storage',
            description: 'S3 bucket name where Terraform state is stored'
        )
        string(
            name: 'S3_KEY_PREFIX',
            defaultValue: 'jenkins-airgap-rke2/terraform.tfstate',
            description: 'S3 key prefix for the Terraform state files'
        )
        string(
            name: 'S3_REGION',
            defaultValue: 'us-east-2',
            description: 'AWS region where the S3 bucket is located'
        )
    }

    environment {
        // Default S3 configuration - will be auto-detected from workspace config
        S3_BUCKET_NAME = "${params.S3_BUCKET_NAME ?: ''}"
        S3_KEY_PREFIX = "${params.S3_KEY_PREFIX ?: ''}"
        S3_REGION = "${params.S3_REGION ?: ''}"
        AWS_REGION = "${params.S3_REGION ?: 'us-east-2'}"

        // Repository configurations
        RANCHER_TEST_REPO_URL = "${params.RANCHER_TEST_REPO_URL ?: 'https://github.com/rancher/tests'}"
        QA_INFRA_REPO = "${params.QA_INFRA_REPO_URL ?: 'https://github.com/rancher/qa-infra-automation'}"
        QA_INFRA_WORK_PATH = '/root/go/src/github.com/rancher/qa-infra-automation'
        ROOT_PATH = '/root/go/src/github.com/rancher/tests/'

        // Computed values
        JOB_SHORT_NAME = "${getShortJobName()}"
        BUILD_CONTAINER_NAME = "${JOB_SHORT_NAME}${BUILD_NUMBER}-destroy"
        IMAGE_NAME = "rancher-destroy-${JOB_SHORT_NAME}${BUILD_NUMBER}"
        VALIDATION_VOLUME = "DestroySharedVolume-${JOB_SHORT_NAME}${BUILD_NUMBER}"

        // Target workspace from parameters
        TARGET_WORKSPACE = "${params.TARGET_WORKSPACE}"
        TF_WORKSPACE = "${params.TARGET_WORKSPACE}"

        // Timeouts (in minutes)
        TERRAFORM_TIMEOUT = '30'

        // Configuration files
        TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
        TERRAFORM_BACKEND_VARS_FILENAME = 'backend.tfvars'
        ENV_FILE = '.env'
    }

    stages {
            stage('Initialize Pipeline') {
                steps {
                    script {
                        logInfo('Initializing pipeline')
                        // Validate parameters and environment
                        validateParameters()

                        // Clean workspace
                        deleteDir()

                        logInfo('Pipeline initialized successfully')
                        logInfo("Build container: ${env.BUILD_CONTAINER_NAME}")
                        logInfo("Docker image: ${env.IMAGE_NAME}")
                        logInfo("Volume: ${env.VALIDATION_VOLUME}")
                    }
                }
            }

            stage('Checkout Repositories') {
                steps {
                    script {
                        logInfo('Checking out source repositories')

                        // Checkout Rancher Tests Repository
                        dir('./tests') {
                            logInfo("Cloning rancher tests repository from ${env.RANCHER_TEST_REPO_URL}")
                            checkout([
                                $class: 'GitSCM',
                                branches: [[name: "*/${params.RANCHER_TEST_REPO_BRANCH}"]],
                                extensions: [
                                    [$class: 'CleanCheckout'],
                                    [$class: 'CloneOption', depth: 1, shallow: true]
                                ],
                                userRemoteConfigs: [[
                                    url: env.RANCHER_TEST_REPO_URL,
                                ]]
                            ])
                        }

                        // Checkout QA Infrastructure Repository
                        dir('./qa-infra-automation') {
                            logInfo("Cloning qa-infra-automation repository from ${env.QA_INFRA_REPO}")
                            checkout([
                                $class: 'GitSCM',
                                branches: [[name: "*/${params.QA_INFRA_REPO_BRANCH}"]],
                                extensions: [
                                    [$class: 'CleanCheckout'],
                                    [$class: 'CloneOption', depth: 1, shallow: true]
                                ],
                                userRemoteConfigs: [[
                                    url: env.QA_INFRA_REPO,
                                ]]
                            ])
                        }

                        logInfo('Repository checkout completed successfully')
                    }
                }
            }

            stage('Configure Environment') {
                steps {
                    script {
                        logInfo('Configuring deployment environment')

                        // Configure credentials and environment files
                        withCredentials(getCredentialsList()) {
                            // Generate environment file with AWS credentials
                            generateDestructionEnvironmentFile()

                            // Setup SSH keys securely
                            setupSSHKeys()

                            // Build Docker image with proper tagging
                            buildDockerImage()

                            // Create shared volume
                            createSharedVolume()
                        }
                    }
                }
            }

            stage('Infrastructure Destruction Operations') {
                steps {
                    script {
                        logInfo('Perfom infrastructure destruction with OpenTofu')

                        // Configuration validation
                        def requiredVars = [
                            'QA_INFRA_WORK_PATH',
                            'TF_WORKSPACE',
                            'TERRAFORM_VARS_FILENAME',
                            'TERRAFORM_BACKEND_VARS_FILENAME',
                            'TERRAFORM_TIMEOUT'
                        ]
                        validateRequiredVariables(requiredVars)

                        // Enhanced timeout with reasonable defaults
                        def timeoutMinutes = env.TERRAFORM_TIMEOUT ?
                            Integer.parseInt(env.TERRAFORM_TIMEOUT) : 30

                        timeout(time: timeoutMinutes, unit: 'MINUTES') {
                            try {
                                // Pre-flight checks
                                validateInfrastructurePrerequisites()

                                // Infrastructure destruction with enhanced error handling
                                performInfrastructureDestruction()

                                // Post-destruction validation
                                validateInfrastructureState()
                            }
                            catch (org.jenkinsci.plugins.workflow.steps.FlowInterruptedException e) {
                            logError("Infrastructure destruction timed out after ${timeoutMinutes} minutes")
                            logError("Timeout exception details: ${e.message}")
                            try {
                                archiveDestructionFailureArtifacts()   
                            } catch (Exception cleanupException) {
                                logError("Failed to perform infrastructure cleanup: ${cleanupException.message}")
                            }
                        }

                        logInfo('Infrastructure destruction process completed.')
                        archiveDestructionResults()
                    }
                }
                post {
                    failure {
                        script {
                            logError('Infrastructure destruction operations failed')
                            archiveDestructionFailureArtifacts()
                        }
                    }
                }
            }
        }
    }

    post {
        always {
            script {
                logInfo('Starting post-destruction cleanup')

                // Archive important artifacts
                archiveBuildArtifacts([
                    'destruction-plan.txt',
                    'destruction-summary.json',
                    'destruction-logs.txt'
                ])

                // Cleanup containers and volumes
                cleanupContainersAndVolumes()
            }
        }

        success {
            script {
                logInfo('Destruction pipeline completed successfully')
                sendSlackNotification([
                    color: 'good',
                    message: "✅ Infrastructure destruction succeeded for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }

        failure {
            script {
                logError('Destruction pipeline failed')
                sendSlackNotification([
                    color: 'danger',
                    message: "❌ Infrastructure destruction failed for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }

        aborted {
            script {
                logWarning('Destruction pipeline was aborted')
                sendSlackNotification([
                    color: 'warning',
                    message: "⚠️ Infrastructure destruction aborted for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }
    }
    
}

/**
 * DESTRUCTION-SPECIFIC HELPER FUNCTIONS
 */

def validateParameters() {
    // Validate required parameters
    if (!params.TARGET_WORKSPACE || params.TARGET_WORKSPACE.trim().isEmpty()) {
        error('TARGET_WORKSPACE parameter is required for destruction')
    }
    if (!params.RANCHER_TEST_REPO_URL) {
        error('RANCHER_TEST_REPO_URL parameter is required')
    }
    if (!params.QA_INFRA_REPO_URL) {
        error('QA_INFRA_REPO_URL parameter is required')
    }

    logInfo('Parameters validated successfully')
    logInfo("Target workspace: ${params.TARGET_WORKSPACE}")
}

def validateRequiredVariables(requiredVars) {
    logInfo('Validating required environment variables')

    def missingVars = []
    requiredVars.each { varName ->
        def varValue = env."${varName}"
        if (!varValue || varValue.trim().isEmpty()) {
            missingVars.add(varName)
        }
    }

    if (!missingVars.isEmpty()) {
        def errorMsg = "Missing required environment variables: ${missingVars.join(', ')}"
        logError(errorMsg)
        throw new IllegalArgumentException(errorMsg)
    }

    logInfo('All required variables validated successfully')
}

def validateInfrastructurePrerequisites() {
    logInfo('Validating infrastructure prerequisites')

    def prerequisiteScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Checking OpenTofu installation...'
tofu version

echo 'Checking workspace directory...'
test -d ${env.QA_INFRA_WORK_PATH}

echo 'Validating terraform vars file...'
test -f ${env.QA_INFRA_WORK_PATH}/tofu/aws/modules/airgap/${env.TERRAFORM_VARS_FILENAME}

echo 'All infrastructure prerequisites validated successfully'
"""

    try {
        executeScriptInContainer(prerequisiteScript)
        logInfo('All infrastructure prerequisites validated')
    } catch (Exception e) {
        def errorMsg = "Infrastructure prerequisites validation failed: ${e.message}"
        logError(errorMsg)
        throw new RuntimeException(errorMsg, e)
    }
}

def validateInfrastructureState() {
    logInfo('Validating infrastructure state after deployment')

    def validationScript = """
cd ${env.QA_INFRA_WORK_PATH}
export TF_WORKSPACE="${env.TF_WORKSPACE}"

echo 'Validating remote terraform state via tofu commands (S3 backend)...'

echo 'Checking key infrastructure resources from remote state...'
tofu -chdir=/root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap state list > /root/state-list.txt 2>&1
STATE_RC=\$?
echo "State list command completed with return code: \$STATE_RC"
if [ \$STATE_RC -ne 0 ]; then
    echo 'ERROR: Failed to retrieve state from remote backend'
    cat /root/state-list.txt
    exit 1
fi

echo 'State retrieval passed'

echo 'Checking if state has no resources...'
STATE_COUNT=\$(wc -l < /root/state-list.txt)
if [ "\$STATE_COUNT" -gt 0 ]; then 
    echo 'WARNING: Resources found in state - this may indicate infrastructure was not destroyed successfully'
    cat /root/state-list.txt
else
    echo "SUCCESS: State contains \$(STATE_COUNT) resources"
    echo 'Sample state resources:'
    head -5 /root/state-list.txt
fi

echo 'Generating and validating outputs from remote state...'
tofu -chdir=/root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap output -json > /root/infrastructure-outputs.json 2>&1
OUTPUT_RC=\$?
if [ \$OUTPUT_RC -gt 0 ]; then
    echo 'ERROR: Failed to generate terraform outputs from remote state'
    cat /root/infrastructure-outputs.json
    exit 1
fi

OUTPUT_SIZE=\$(stat -c%s /root/infrastructure-outputs.json 2>/dev/null || echo 0)
if [ "\$OUTPUT_SIZE" -gt 0 ]; then
    echo 'ERROR: Outputs file is not empty (\$OUTPUT_SIZE bytes)'
else
    echo 'SUCCESS: Outputs file is empty'
fi

echo 'Infrastructure state validation completed successfully'
    """

    executeScriptInContainer(validationScript)
    logInfo('Infrastructure state validated successfully')
}

def getShortJobName() {
    def jobName = "${env.JOB_NAME}"
    if (jobName.contains('/')) {
        def lastSlashIndex = jobName.lastIndexOf('/')
        return jobName.substring(lastSlashIndex + 1)
    }
    return jobName
}

def getCredentialsList() {
    return [
        string(credentialsId: 'AWS_ACCESS_KEY_ID', variable: 'AWS_ACCESS_KEY_ID'),
        string(credentialsId: 'AWS_SECRET_ACCESS_KEY', variable: 'AWS_SECRET_ACCESS_KEY'),
        string(credentialsId: 'SLACK_WEBHOOK', variable: 'SLACK_WEBHOOK')
    ]
}

def generateDestructionEnvironmentFile() {
    logInfo('Generating environment file for destruction containers')

    // Build environment content securely without direct interpolation of secrets
    def envLines = [
        '# Environment variables for infrastructure destruction containers',
        "TARGET_WORKSPACE=${env.TARGET_WORKSPACE}",
        "BUILD_NUMBER=${env.BUILD_NUMBER}",
        "JOB_NAME=${env.JOB_NAME}",
        "QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH}",
        "TERRAFORM_VARS_FILENAME=${env.TERRAFORM_VARS_FILENAME}",
        "S3_BUCKET_NAME=${env.S3_BUCKET_NAME}",
        "S3_KEY_PREFIX=${env.S3_KEY_PREFIX}",
        "S3_REGION=${env.S3_REGION}",
        "AWS_REGION=${env.AWS_REGION}",
        '',
        '# AWS Credentials for OpenTofu'
    ]

    // Add credentials securely
    envLines.add('AWS_ACCESS_KEY_ID=' + env.AWS_ACCESS_KEY_ID)
    envLines.add('AWS_SECRET_ACCESS_KEY=' + env.AWS_SECRET_ACCESS_KEY)
    envLines.add('')
    envLines.add('# Terraform Variables for OpenTofu (TF_VAR_ prefix for automatic variable population)')
    envLines.add('TF_VAR_aws_access_key=' + env.AWS_ACCESS_KEY_ID)
    envLines.add('TF_VAR_aws_secret_access_key=' + env.AWS_SECRET_ACCESS_KEY)
    envLines.add('TF_VAR_aws_region=' + env.AWS_REGION)

    def envContent = envLines.join('\n')
    writeFile file: env.ENV_FILE, text: envContent
    logInfo("Environment file created: ${env.ENV_FILE}")
}

def setupSSHKeys() {
    if (env.AWS_SSH_PEM_KEY && env.AWS_SSH_KEY_NAME) {
        logInfo('Setting up SSH keys')

        dir('./tests/.ssh') {
            def decodedKey = new String(env.AWS_SSH_PEM_KEY.decodeBase64())
            writeFile file: env.AWS_SSH_KEY_NAME, text: decodedKey
            sh "chmod 600 ${env.AWS_SSH_KEY_NAME}"
        }

        logInfo('SSH keys configured successfully')
    }
}

def buildDockerImage() {
    logInfo("Building Docker image: ${env.IMAGE_NAME}")

    dir('./') {
        sh './tests/validation/configure.sh'
        sh """
            docker build . \
                -f ./tests/validation/Dockerfile.tofu.e2e \
                -t ${env.IMAGE_NAME} \
                --build-arg BUILD_DATE=\$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
                --build-arg VCS_REF=\$(git rev-parse --short HEAD) \
                --label "pipeline.build.number=${env.BUILD_NUMBER}" \
                --label "pipeline.job.name=${env.JOB_NAME}"
        """
    }

    logInfo('Docker image built successfully')
}

def createSharedVolume() {
    logInfo("Creating shared volume: ${env.VALIDATION_VOLUME}")
    sh "docker volume create --name ${env.VALIDATION_VOLUME}"
}

def executeScriptInContainer(scriptContent, extraEnv = [:], skipWorkspaceEnv = false) {
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-script-${timestamp}"
    def scriptFile = "docker-script-${timestamp}.sh"

    writeFile file: scriptFile, text: scriptContent

    def envVars = ''
    extraEnv.each { key, value ->
        envVars += " -e ${key}=${value}"
    }

    def workspaceEnv = skipWorkspaceEnv ? '' : " -e TF_WORKSPACE=${env.TF_WORKSPACE}"

    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/qa-infra-automation:/root/go/src/github.com/rancher/qa-infra-automation \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=/root/go/src/github.com/rancher/qa-infra-automation${workspaceEnv} \
            ${envVars} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def downloadConfigurationFromS3() {
    logInfo('Downloading configuration files from S3')

    def downloadScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Creating configuration directory...'
# cspell:ignore airgap
mkdir -p tofu/aws/modules/airgap

# cspell:ignore tfvars
echo 'Downloading cluster.tfvars from S3...'
aws s3 cp \\
    s3://"${env.S3_BUCKET_NAME}"/env:/"${env.TF_WORKSPACE}"/config/"${env.TERRAFORM_VARS_FILENAME}" \\
    tofu/aws/modules/airgap/"${env.TERRAFORM_VARS_FILENAME}" \\
    --region "${env.S3_REGION}"

if [ \$? -eq 0 ]; then
    echo 'SUCCESS: cluster.tfvars downloaded from S3'
    ls -la tofu/aws/modules/airgap/"${env.TERRAFORM_VARS_FILENAME}"
else
    echo 'ERROR: Failed to download cluster.tfvars from S3'
    echo 'Available files in S3 config directory:'
    aws s3 ls \\
        s3://"${env.S3_BUCKET_NAME}"/env:/"${env.TF_WORKSPACE}"/config/" \\
        --region "${env.S3_REGION}" || echo 'Failed to list S3 contents'
    exit 1
fi
echo 'Configuration files downloaded successfully'
echo 'Verifying downloaded files:'
ls -la tofu/aws/modules/airgap/*.tfvars
    """

    executeScriptInContainer(downloadScript)
    logInfo('Configuration files downloaded from S3 successfully')
}

def generateTofuConfiguration() {
    logInfo('Generating OpenTofu configuration files')

    def terraformConfig = readFile file: "${env.QA_INFRA_WORK_PATH}/tofu/aws/modules/airgap/${env.TERRAFORM_VARS_FILENAME}"
    logInfo("Terraform configuration loaded from ${env.TERRAFORM_VARS_FILENAME}")

    terraformConfig = terraformConfig.replace('${AWS_SECRET_ACCESS_KEY}', env.AWS_SECRET_ACCESS_KEY ?: '')
    terraformConfig = terraformConfig.replace('${AWS_ACCESS_KEY_ID}', env.AWS_ACCESS_KEY_ID ?: '')

    dir('./qa-infra-automation') {
        dir('./tofu/aws/modules/airgap') {
            writeFile file: env.TERRAFORM_VARS_FILENAME, text: terraformConfig
            logInfo("Terraform configuration written to: ${env.TERRAFORM_VARS_FILENAME}")
        }
    }
}

def generateTofuBackendConfiguration() {
    logInfo('Generating Tofu configuration')

    // Ensure S3 backend parameters are set
    if (!env.S3_BUCKET_NAME) { throw new RuntimeException('S3_BUCKET_NAME environment variable is not set') }
    if (!env.S3_REGION) { throw new RuntimeException('S3_REGION environment variable is not set') }
    if (!env.S3_KEY_PREFIX) { throw new RuntimeException('S3_KEY_PREFIX environment variable is not set') }

    sh 'mkdir -p qa-infra-automation/tofu/aws/modules/airgap'

     // Write the configuration file
    dir('./qa-infra-automation') {
        dir('./tofu/aws/modules/airgap') {

            // Generate backend.tfvars content from S3 parameters
            def backendVars = """
bucket = \"${env.S3_BUCKET_NAME}\"
key    = \"${env.S3_KEY_PREFIX}"
region = \"${env.S3_REGION}\"
"""
            writeFile file: env.TERRAFORM_BACKEND_VARS_FILENAME, text: backendVars
            logInfo("S3 backend configuration written to: ${env.TERRAFORM_BACKEND_VARS_FILENAME}")
        }
    }
}

def performInfrastructureDestruction() {
    downloadConfigurationFromS3()
    generateTofuConfiguration()
    generateTofuBackendConfiguration()
    initializeOpenTofu()
    manageWorkspace()
    executeDestruction()

    logInfo('Infrastructure destruction completed successfully')
}

def initializeOpenTofu() {
    logInfo('Initializing OpenTofu with S3 backend')

    def initScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Initializing OpenTofu with S3 backend configuration...'
tofu -chdir=tofu/aws/modules/airgap init -backend-config="${env.TERRAFORM_BACKEND_VARS_FILENAME}" -input=false -upgrade

echo 'Verifying initialization success...'
tofu -chdir=tofu/aws/modules/airgap providers

echo 'Creating/selecting target workspace: ${env.TF_WORKSPACE}'
tofu -chdir=tofu/aws/modules/airgap workspace select "${env.TF_WORKSPACE}" || tofu -chdir=tofu/aws/modules/airgap workspace new "${env.TF_WORKSPACE}"

echo 'Verifying workspace selection...'
CURRENT_WORKSPACE=\$(tofu -chdir=tofu/aws/modules/airgap workspace show)
echo "Current workspace: \$CURRENT_WORKSPACE"

if [ "\$CURRENT_WORKSPACE" != "${env.TF_WORKSPACE}" ]; then
    echo "ERROR: Expected workspace ${env.TF_WORKSPACE}, but got '\$CURRENT_WORKSPACE'"
    echo 'Available workspaces:'
    tofu -chdir=tofu/aws/modules/airgap workspace list
    exit 1
fi

echo 'OpenTofu initialization and workspace setup completed successfully'
    """

    executeScriptInContainer(initScript, [:], true)
}

def manageWorkspace() {
    logInfo("Managing OpenTofu workspace: ${env.TF_WORKSPACE}")

    def workspaceScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Managing workspace state...'
unset TF_WORKSPACE

echo 'Current workspaces:'
tofu -chdir=tofu/aws/modules/airgap workspace list

echo 'Creating or selecting workspace: ${env.TF_WORKSPACE}'
if ! tofu -chdir=tofu/aws/modules/airgap workspace select "${env.TF_WORKSPACE}" 2>/dev/null; then
    echo 'Workspace does not exist, creating new workspace...'
    tofu -chdir=tofu/aws/modules/airgap workspace new "${env.TF_WORKSPACE}"

    if ! tofu -chdir=tofu/aws/modules/airgap workspace select "${env.TF_WORKSPACE}"; then
        echo 'ERROR: Failed to create and select workspace'
        exit 1
    fi
fi

# Verify workspace selection
CURRENT_WORKSPACE=\$(tofu -chdir=tofu/aws/modules/airgap workspace show)
echo "Current workspace: \$CURRENT_WORKSPACE"

# Strip whitespace and handle empty responses
CURRENT_WORKSPACE=\$(echo "\$CURRENT_WORKSPACE" | xargs)

if [ "\$CURRENT_WORKSPACE" = "" ]; then
    echo 'ERROR: Workspace show command returned empty response'
    tofu -chdir=tofu/aws/modules/airgap workspace list
    exit 1
fi

if [ "\$CURRENT_WORKSPACE" != "${env.TF_WORKSPACE}" ]; then
    echo "ERROR: Expected workspace ${env.TF_WORKSPACE}, but got '\$CURRENT_WORKSPACE'"
    echo 'Available workspaces:'
    tofu -chdir=tofu/aws/modules/airgap workspace list
    exit 1
fi

export TF_WORKSPACE="${env.TF_WORKSPACE}"
echo "Workspace management completed: \$TF_WORKSPACE"

tofu -chdir=tofu/aws/modules/airgap init -input=false -upgrade
    """

    executeScriptInContainer(workspaceScript)
}


def executeDestruction() {
    logInfo('Executing direct infrastructure destruction')

    def destructionScript = """
    cd ${env.QA_INFRA_WORK_PATH}
    export TF_WORKSPACE="${env.TF_WORKSPACE}"

    echo 'Debug: Listing current directory contents...'
    ls -la .

    echo 'Debug: Listing mounted qa-infra-automation/tofu/aws/modules/airgap contents...'
    ls -la /root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap/ || echo 'Mounted directory listing failed'

    echo "--- Starting Direct Destruction ---"
    # Pass placeholder variables directly to the destroy command to satisfy required variables
    tofu -chdir=/root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap destroy -auto-approve -input=false -var-file="${env.TERRAFORM_VARS_FILENAME}"

    echo "--- Verifying Destruction ---"
    REMAINING_RESOURCES=\$(tofu -chdir=/root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap state list | wc -l || echo 0)
    if [ "\$REMAINING_RESOURCES" -eq 0 ]; then
        echo "✅ Success: All resources have been destroyed."
        # Clean up the workspace
        echo "--- Cleaning up Terraform Workspace ---"
        tofu -chdir=/root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap workspace select default || echo "Could not switch to default workspace."
        tofu -chdir=/root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap workspace delete "${env.TARGET_WORKSPACE}" || echo "Could not delete workspace ${env.TARGET_WORKSPACE}."
    else
        echo "❌ WARNING: \$REMAINING_RESOURCES resources still remain in state."
        tofu -chdir=/root/go/src/github.com/rancher/qa-infra-automation/tofu/aws/modules/airgap state list
        exit 1
    fi
    """

    executeScriptInContainer(destructionScript)
}

def archiveDestructionResults() {
    logInfo('Archiving destruction results')

    try {
        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/destruction-summary.json ./ || true
        """

        logInfo('Destruction results archived')
    } catch (Exception e) {
        logError("Failed to archive destruction results: ${e.message}")
    }
}

def archiveDestructionFailureArtifacts() {
    logInfo('Archiving destruction failure artifacts')

    try {
        def debugCommands = [
            "cd ${env.QA_INFRA_WORK_PATH}",
            'tofu -chdir=tofu/aws/modules/airgap workspace list > workspace-list.txt 2>&1 || echo "No workspace list available"',
            "tofu -chdir=tofu/aws/modules/airgap state list > remaining-resources.txt 2>&1 || echo 'No state available'",
            "echo 'Destruction failure artifact collection completed'"
        ]

        executeInContainer(debugCommands)

        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/workspace-list.txt ./ || true
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/remaining-resources.txt ./ || true
        """

        archiveArtifacts artifacts: 'workspace-list.txt,remaining-resources.txt', allowEmptyArchive: true
    } catch (Exception e) {
        logError("Failed to archive failure artifacts: ${e.message}")
    }
}

def executeInContainer(commands) {
    def commandString = commands.join(' && ')
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"
    def scriptFile = "destroy-commands-${timestamp}.sh"

    writeFile file: scriptFile, text: commandString

    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            -e TF_WORKSPACE=${env.TARGET_WORKSPACE} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def cleanupContainersAndVolumes() {
    logInfo('Cleaning up Docker containers and volumes')

    try {
        sh """
            # Stop and remove containers
            docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker stop || true
            docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker rm -v || true

            # Remove the Docker image
            docker rmi -f ${env.IMAGE_NAME} || true

            # Remove the shared volume
            docker volume rm -f ${env.VALIDATION_VOLUME} || true

            # Clean up dangling resources
            docker system prune -f || true
        """
    } catch (Exception e) {
        logError("Docker cleanup failed: ${e.message}")
    }
}

def archiveBuildArtifacts(artifacts) {
    try {
        archiveArtifacts artifacts: artifacts.join(','), allowEmptyArchive: true
        logInfo("Artifacts archived: ${artifacts.join(', ')}")
    } catch (Exception e) {
        logError("Failed to archive artifacts: ${e.message}")
    }
}

def sendSlackNotification(config) {
    if (env.SLACK_WEBHOOK) {
        try {
            def payload = [
                channel: '#rancher-qa',
                username: 'Jenkins-Destroyer',
                color: config.color,
                title: 'Infrastructure Destruction Pipeline',
                message: config.message,
                fields: [
                    [title: 'Job', value: env.JOB_NAME, short: true],
                    [title: 'Build', value: env.BUILD_NUMBER, short: true],
                    [title: 'Workspace', value: env.TARGET_WORKSPACE, short: true],
                    [title: 'State Source', value: 'S3 Backend', short: true]
                ]
            ]

            httpRequest(
                httpMode: 'POST',
                url: env.SLACK_WEBHOOK,
                contentType: 'APPLICATION_JSON',
                requestBody: groovy.json.JsonOutput.toJson(payload)
            )

            logInfo('Slack notification sent successfully')
        } catch (Exception e) {
            logError("Failed to send Slack notification: ${e.message}")
        }
    }
}

/**
 * LOGGING FUNCTIONS
 */

def logInfo(message) {
    echo "ℹ️ [INFO] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logError(message) {
    echo "❌ [ERROR] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logWarning(message) {
    echo "⚠️ [WARNING] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logDebug(message) {
    if (params.LOG_LEVEL == 'DEBUG' || params.LOG_LEVEL == 'VERBOSE') {
        echo "🔍 [DEBUG] ${new Date().format('HH:mm:ss')} - ${message}"
    }
}
