#!/usr/bin/env groovy

/**
 * Infrastructure Destruction Jenkinsfile for Airgap RKE2
 *
 * This pipeline is designed to safely destroy infrastructure created by
 * the main airgap RKE2 deployment pipeline. It retrieves Terraform state
 * from S3 backend and performs controlled destruction.
 */

pipeline {
    agent any

    // Global pipeline options
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timeout(time: 2, unit: 'HOURS')
        timestamps()
        ansiColor('xterm')
        skipStagesAfterUnstable()
    }

    parameters {
        string(
            name: 'RANCHER_TEST_REPO_URL',
            defaultValue: 'https://github.com/rancher/tests',
            description: 'URL of rancher/tests repository'
        )
        string(
            name: 'RANCHER_TEST_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of rancher/tests repository'
        )
        string(
            name: 'QA_INFRA_REPO_URL',
            defaultValue: 'https://github.com/rancher/qa-infra-automation',
            description: 'URL of qa-infra-automation repository'
        )
        string(
            name: 'QA_INFRA_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of qa-infra-automation repository'
        )
        string(
            name: 'S3_BUCKET_NAME',
            defaultValue: 'jenkins-terraform-state-storage',
            description: 'S3 bucket name where Terraform state is stored'
        )
        string(
            name: 'S3_KEY_PREFIX',
            defaultValue: 'jenkins-airgap-rke2',
            description: 'S3 key prefix for the Terraform state files'
        )
        string(
            name: 'S3_REGION',
            defaultValue: 'us-east-2',
            description: 'AWS region where the S3 bucket is located'
        )
        string(
            name: 'TARGET_WORKSPACE',
            defaultValue: '',
            description: 'Terraform workspace to destroy (e.g., jenkins_airgap_ansible_workspace_123)'
        )
    }

    environment {
        // Dynamic state storage configuration from parameters
        TF_STATE_BUCKET = "${params.S3_BUCKET_NAME ?: 'jenkins-terraform-state-storage'}"
        TF_STATE_KEY_PREFIX = "${params.S3_KEY_PREFIX ?: 'jenkins-airgap-rke2'}"
        TF_STATE_REGION = "${params.S3_REGION ?: 'us-east-2'}"

        // Repository configurations
        RANCHER_TEST_REPO_URL = "${params.RANCHER_TEST_REPO_URL ?: 'https://github.com/rancher/tests'}"
        QA_INFRA_REPO = "${params.QA_INFRA_REPO_URL ?: 'https://github.com/rancher/qa-infra-automation'}"
        QA_INFRA_WORK_PATH = '/root/go/src/github.com/rancher/qa-infra-automation'
        ROOT_PATH = '/root/go/src/github.com/rancher/tests/'

        // Computed values
        JOB_SHORT_NAME = "${getShortJobName()}"
        BUILD_CONTAINER_NAME = "${JOB_SHORT_NAME}${BUILD_NUMBER}-destroy"
        IMAGE_NAME = "rancher-destroy-${JOB_SHORT_NAME}${BUILD_NUMBER}"
        VALIDATION_VOLUME = "DestroySharedVolume-${JOB_SHORT_NAME}${BUILD_NUMBER}"

        // Target workspace from parameters
        TARGET_WORKSPACE = "${params.TARGET_WORKSPACE}"

        // Configuration files
        TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
        ENV_FILE = '.env-destroy'

        // AWS defaults
        AWS_REGION = 'us-east-2'
    }

    stages {
        stage('Infrastructure Destruction Operations') {
            steps {
                script {
                    logWarning('⚠️ IMPORTANT: No concurrent state locking available')
                    logWarning('⚠️ Ensure only ONE destruction pipeline runs at a time for the same workspace')
                    logWarning('⚠️ Manual coordination required to prevent state conflicts')

                    // Validate Parameters
                    logInfo('Validating destruction parameters')
                    if (!params.TARGET_WORKSPACE || params.TARGET_WORKSPACE.trim().isEmpty()) {
                        error('TARGET_WORKSPACE parameter is required for destruction')
                    }
                    env.TARGET_WORKSPACE = params.TARGET_WORKSPACE
                    logInfo("Target workspace: ${env.TARGET_WORKSPACE}")
                    logInfo('State source: S3 Backend')

                    // Prepare Environment
                    logInfo('Preparing destruction environment')
                    deleteDir()

                    // Checkout repositories
                    dir('./qa-infra-automation') {
                        logInfo('Cloning qa-infra-automation repository')
                        checkout([$class: 'GitSCM', branches: [[name: "*/${params.QA_INFRA_REPO_BRANCH}"]], extensions: [[$class: 'CleanCheckout'], [$class: 'CloneOption', depth: 1, shallow: true]], userRemoteConfigs: [[url: env.QA_INFRA_REPO]]])
                    }
                    dir('./tests') {
                        logInfo('Cloning tests repository for Docker build')
                        checkout([$class: 'GitSCM', branches: [[name: "*/${params.RANCHER_TEST_REPO_BRANCH}"]], extensions: [[$class: 'CleanCheckout'], [$class: 'CloneOption', depth: 1, shallow: true]], userRemoteConfigs: [[url: env.RANCHER_TEST_REPO_URL,]]])
                    }

                    // Configure credentials and build environment
                    withCredentials(getCredentialsList()) {
                        generateDestructionEnvironmentFile()
                        setupSSHKeys()
                        buildDestructionDockerImage()
                        createSharedVolume()
                    }

                    // Retrieve Terraform State
                    logInfo('Retrieving Terraform state from S3 backend for destruction')
                    def stateRetrieved = retrieveStateFromS3()
                    if (!stateRetrieved) {
                        error('Could not retrieve Terraform state from S3. Halting destruction.')
                    }


                    // Execute the destruction
                    executeDestruction()
                    logInfo('Infrastructure destruction process completed.')
                    archiveDestructionResults()
                }
            }
            post {
                failure {
                    script {
                        logError('Infrastructure destruction operations failed')
                        archiveDestructionFailureArtifacts()
                    }
                }
            }
        }
    }

    post {
        always {
            script {
                logInfo('Starting post-destruction cleanup')

                // Archive important artifacts
                archiveBuildArtifacts([
                    'destruction-plan.txt',
                    'destruction-summary.json',
                    'destruction-logs.txt'
                ])

                // Cleanup containers and volumes
                cleanupContainersAndVolumes()
            }
        }

        success {
            script {
                logInfo('Destruction pipeline completed successfully')
                sendSlackNotification([
                    color: 'good',
                    message: "✅ Infrastructure destruction succeeded for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }

        failure {
            script {
                logError('Destruction pipeline failed')
                sendSlackNotification([
                    color: 'danger',
                    message: "❌ Infrastructure destruction failed for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }

        aborted {
            script {
                logWarning('Destruction pipeline was aborted')
                sendSlackNotification([
                    color: 'warning',
                    message: "⚠️ Infrastructure destruction aborted for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }
    }
}

/**
 * DESTRUCTION-SPECIFIC HELPER FUNCTIONS
 */

def getShortJobName() {
    def jobName = "${env.JOB_NAME}"
    if (jobName.contains('/')) {
        def lastSlashIndex = jobName.lastIndexOf('/')
        return jobName.substring(lastSlashIndex + 1)
    }
    return jobName
}

def getCredentialsList() {
    return [
        string(credentialsId: 'AWS_ACCESS_KEY_ID', variable: 'AWS_ACCESS_KEY_ID'),
        string(credentialsId: 'AWS_SECRET_ACCESS_KEY', variable: 'AWS_SECRET_ACCESS_KEY'),
        string(credentialsId: 'SLACK_WEBHOOK', variable: 'SLACK_WEBHOOK')
    ]
}

def generateDestructionEnvironmentFile() {
    logInfo('Generating environment file for destruction containers')

    // Build environment content securely without direct interpolation of secrets
    def envLines = [
        '# Environment variables for infrastructure destruction containers',
        "TARGET_WORKSPACE=${env.TARGET_WORKSPACE}",
        "BUILD_NUMBER=${env.BUILD_NUMBER}",
        "JOB_NAME=${env.JOB_NAME}",
        "QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH}",
        "TERRAFORM_VARS_FILENAME=${env.TERRAFORM_VARS_FILENAME}",
        "TF_STATE_BUCKET=${env.TF_STATE_BUCKET}",
        "TF_STATE_KEY_PREFIX=${env.TF_STATE_KEY_PREFIX}",
        "TF_STATE_REGION=${env.TF_STATE_REGION}",
        "AWS_REGION=${env.AWS_REGION}",
        '',
        '# AWS Credentials for OpenTofu'
    ]

    // Add credentials securely
    envLines.add('AWS_ACCESS_KEY_ID=' + env.AWS_ACCESS_KEY_ID)
    envLines.add('AWS_SECRET_ACCESS_KEY=' + env.AWS_SECRET_ACCESS_KEY)
    envLines.add('')
    envLines.add('# Terraform Variables for OpenTofu (TF_VAR_ prefix for automatic variable population)')
    envLines.add('TF_VAR_aws_access_key=' + env.AWS_ACCESS_KEY_ID)
    envLines.add('TF_VAR_aws_secret_access_key=' + env.AWS_SECRET_ACCESS_KEY)
    envLines.add('TF_VAR_aws_region=' + env.AWS_REGION)

    def envContent = envLines.join('\n')
    writeFile file: env.ENV_FILE, text: envContent
    logInfo("Environment file created: ${env.ENV_FILE}")
}

def setupSSHKeys() {
    if (env.AWS_SSH_PEM_KEY && env.AWS_SSH_KEY_NAME) {
        logInfo('Setting up SSH keys')

        dir('./tests/.ssh') {
            def decodedKey = new String(env.AWS_SSH_PEM_KEY.decodeBase64())
            writeFile file: env.AWS_SSH_KEY_NAME, text: decodedKey
            sh "chmod 600 ${env.AWS_SSH_KEY_NAME}"
        }

        logInfo('SSH keys configured successfully')
    }
}

def buildDestructionDockerImage() {
    logInfo("Building destruction Docker image: ${env.IMAGE_NAME}")

    dir('./') {
        sh './tests/validation/configure.sh || echo "Configure script not found, continuing..."'
        sh """
            # Use the same Dockerfile as the main pipeline for consistency
            docker build . \
                -f ./tests/validation/Dockerfile.tofu.e2e \
                -t ${env.IMAGE_NAME} \
                --build-arg BUILD_DATE=\$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
                --build-arg VCS_REF=\$(git rev-parse --short HEAD 2>/dev/null || echo 'unknown') \
                --label "pipeline.build.number=${env.BUILD_NUMBER}" \
                --label "pipeline.job.name=${env.JOB_NAME}" \
                --label "pipeline.purpose=destruction"
        """
    }

    logInfo('Destruction Docker image built successfully')
}

def createSharedVolume() {
    logInfo("Creating shared volume: ${env.VALIDATION_VOLUME}")
    sh "docker volume create --name ${env.VALIDATION_VOLUME}"
}

def executeScriptInContainer(scriptContent, extraEnv = [:]) {
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"
    def scriptFile = "destroy-script-${timestamp}.sh"

    writeFile file: scriptFile, text: scriptContent

    def envVars = ''
    extraEnv.each { key, value ->
        envVars += " -e ${key}=${value}"
    }

    // Note: TF_WORKSPACE is intentionally NOT set here to avoid workspace conflicts during init
    // The script itself will manage workspace selection after backend initialization
    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            ${envVars} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def executeScriptInContainerWithWorkspace(scriptContent, extraEnv = [:]) {
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"
    def scriptFile = "destroy-script-${timestamp}.sh"

    writeFile file: scriptFile, text: scriptContent

    def envVars = ''
    extraEnv.each { key, value ->
        envVars += " -e ${key}=${value}"
    }

    // This version sets TF_WORKSPACE for operations that need it (like planning and applying)
    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            -e TF_WORKSPACE=${env.TARGET_WORKSPACE} \
            ${envVars} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def retrieveStateFromS3() {
    logInfo("Enhanced state retrieval with alternative S3 key pattern detection")

    try {
        def retrieveScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo '🔍 Enhanced State Retrieval for Workspace: ${env.TARGET_WORKSPACE}'
echo '=============================================================='
echo "S3 bucket: ${env.TF_STATE_BUCKET}"
echo "Key prefix: ${env.TF_STATE_KEY_PREFIX}"
echo "Region: ${env.TF_STATE_REGION}"
echo "Target workspace: ${env.TARGET_WORKSPACE}"
echo

# Step 1: Use correct S3 key pattern
echo 'Step 1: Using correct S3 key pattern...'
mkdir -p tofu/aws/modules/airgap/

# Use the correct S3 key pattern: prefix/workspace/terraform.tfstate
ACTUAL_STATE_KEY="${env.TF_STATE_KEY_PREFIX}/${env.TARGET_WORKSPACE}/terraform.tfstate"

echo "🎯 Using confirmed S3 key pattern: \$ACTUAL_STATE_KEY"
echo "Full S3 path: s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY"

# Verify the exact state file exists
echo "🔍 Verifying state file exists at confirmed location..."
if aws s3 ls "s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} >/dev/null 2>&1; then
    echo "✅ CONFIRMED: State file exists at exact location"
    
    # Get file metadata
    STATE_METADATA=\$(aws s3api head-object --bucket "${env.TF_STATE_BUCKET}" --key "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} 2>/dev/null)
    STATE_SIZE=\$(echo "\$STATE_METADATA" | python3 -c "import json,sys; data=json.load(sys.stdin); print(data.get('ContentLength', 0))" 2>/dev/null || echo 0)
    LAST_MODIFIED=\$(echo "\$STATE_METADATA" | python3 -c "import json,sys; data=json.load(sys.stdin); print(data.get('LastModified', 'unknown'))" 2>/dev/null || echo "unknown")
    
    echo "State file details:"
    echo "  📏 Size: \$STATE_SIZE bytes"
    echo "  🕐 Modified: \$LAST_MODIFIED"
    
    if [ "\$STATE_SIZE" -eq 0 ]; then
        echo "⚠️ WARNING: State file is empty (0 bytes)"
        echo "This indicates one of the following scenarios:"
        echo "  1. Deployment failed after creating state file"
        echo "  2. Infrastructure was already destroyed"
        echo "  3. State file was corrupted or truncated"
        echo
        
        # Check for S3 versioning to find previous state versions
        echo "🔍 Checking for previous state file versions..."
        VERSIONS=\$(aws s3api list-object-versions \
            --bucket "${env.TF_STATE_BUCKET}" \
            --prefix "\$ACTUAL_STATE_KEY" \
            --region ${env.TF_STATE_REGION} \
            --query 'Versions[?Size > `0`].[VersionId,Size,LastModified]' \
            --output table 2>/dev/null || echo "")
        
        if [ -n "\$VERSIONS" ] && [ "\$VERSIONS" != "None" ]; then
            echo "✅ Found previous non-empty state versions:"
            echo "\$VERSIONS"
            
            # Get the latest non-empty version
            LATEST_VERSION=\$(aws s3api list-object-versions --bucket "${env.TF_STATE_BUCKET}" --prefix "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} --query 'Versions[?Size > `0`] | sort_by(@, &LastModified) | [-1].VersionId' --output text 2>/dev/null || echo "")
            
            if [ -n "\$LATEST_VERSION" ] && [ "\$LATEST_VERSION" != "None" ]; then
                echo "🔄 Attempting to restore from latest non-empty version: \$LATEST_VERSION"
                
                if aws s3api copy-object --bucket "${env.TF_STATE_BUCKET}" --copy-source "${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY?versionId=\$LATEST_VERSION" --key "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} >/dev/null 2>&1; then
                    echo "✅ State file restored from version \$LATEST_VERSION"
                    
                    # Re-check the restored file
                    RESTORED_METADATA=\$(aws s3api head-object --bucket "${env.TF_STATE_BUCKET}" --key "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} 2>/dev/null)
                    RESTORED_SIZE=\$(echo "\$RESTORED_METADATA" | python3 -c "import json,sys; data=json.load(sys.stdin); print(data.get('ContentLength', 0))" 2>/dev/null || echo 0)
                    
                    if [ "\$RESTORED_SIZE" -gt 0 ]; then
                        echo "✅ Restored state file size: \$RESTORED_SIZE bytes"
                        STATE_SIZE=\$RESTORED_SIZE
                    else
                        echo "❌ Restored state file is still empty"
                    fi
                else
                    echo "❌ Failed to restore state from version \$LATEST_VERSION"
                fi
            fi
        else
            echo "❌ No previous non-empty state versions found"
        fi
        
        # If still empty after restoration attempt
        if [ "\$STATE_SIZE" -eq 0 ]; then
            echo
            echo "🔍 Checking for orphaned AWS resources..."
            echo "Searching for resources that might exist despite empty state:"
            
            # Check for common resource patterns that might exist
            echo "Checking for EC2 instances with workspace tag..."
            INSTANCES=\$(aws ec2 describe-instances --region ${env.TF_STATE_REGION} --filters "Name=tag:workspace,Values=${env.TARGET_WORKSPACE}" --query 'Reservations[*].Instances[*].[InstanceId,State.Name,Tags[?Key==`Name`].Value|[0]]' --output table 2>/dev/null || echo "No instances found")
            
            if [ "\$INSTANCES" != "No instances found" ] && [ -n "\$INSTANCES" ]; then
                echo "⚠️ Found EC2 instances that may be orphaned:"
                echo "\$INSTANCES"
                ORPHANED_RESOURCES=true
            else
                echo "No EC2 instances found with workspace tag"
            fi
            
            echo "Checking for VPCs with workspace tag..."
            VPCS=\$(aws ec2 describe-vpcs --region ${env.TF_STATE_REGION} --filters "Name=tag:workspace,Values=${env.TARGET_WORKSPACE}" --query 'Vpcs[*].[VpcId,State,Tags[?Key==`Name`].Value|[0]]' --output table 2>/dev/null || echo "No VPCs found")
            
            if [ "\$VPCS" != "No VPCs found" ] && [ -n "\$VPCS" ]; then
                echo "⚠️ Found VPCs that may be orphaned:"
                echo "\$VPCS"
                ORPHANED_RESOURCES=true
            else
                echo "No VPCs found with workspace tag"
            fi
            
            echo
            if [ "\$ORPHANED_RESOURCES" = "true" ]; then
                echo "⚠️ ORPHANED RESOURCES DETECTED"
                echo "Found AWS resources that may belong to this workspace but are not in Terraform state."
                echo "These resources may need manual cleanup."
                echo
                echo "Recommended actions:"
                echo "1. Review the resources listed above"
                echo "2. Manually delete orphaned resources if confirmed"
                echo "3. Check deployment logs for what went wrong"
                echo
                if [ "${params.FORCE_DESTROY ?: 'false'}" = "true" ]; then
                    echo "⚠️ FORCE_DESTROY is enabled, but no state resources to destroy"
                    echo "Manual cleanup of orphaned resources may be required"
                    exit 0
                else
                    echo "❌ Empty state with potential orphaned resources - manual intervention required"
                    exit 1
                fi
            else
                echo "✅ No orphaned resources found - infrastructure appears to be fully cleaned up"
                echo "The empty state file indicates successful cleanup or deployment failure"
                echo
                if [ "${params.FORCE_DESTROY ?: 'false'}" = "true" ]; then
                    echo "✅ FORCE_DESTROY enabled - treating empty state as successful cleanup"
                    exit 0
                else
                    echo "ℹ️ No resources to destroy - workspace appears clean"
                    echo "This may indicate:"
                    echo "  - Infrastructure was already destroyed"
                    echo "  - Original deployment never completed"
                    echo "  - This is the expected state"
                    exit 0
                fi
            fi
        fi
    fi
    
else
    echo "❌ ERROR: State file not found at confirmed location"
    echo "Expected: s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY"
    
    # Fallback: search for any files with this workspace name
    echo "🔍 Searching for alternative locations..."
    SEARCH_RESULTS=\$(aws s3 ls s3://${env.TF_STATE_BUCKET}/ --region ${env.TF_STATE_REGION} --recursive | grep "${env.TARGET_WORKSPACE}" | grep "terraform.tfstate" || echo "")
    
    if [ -n "\$SEARCH_RESULTS" ]; then
        echo "Found state files matching workspace pattern:"
        echo "\$SEARCH_RESULTS"
        
        # Use the first match as fallback
        ACTUAL_STATE_KEY=\$(echo "\$SEARCH_RESULTS" | head -1 | awk '{print \$4}')
        STATE_SIZE=\$(echo "\$SEARCH_RESULTS" | head -1 | awk '{print \$3}')
        echo "Using fallback state file: \$ACTUAL_STATE_KEY"
        
        if [ "\$STATE_SIZE" -eq 0 ]; then
            echo "❌ ERROR: Fallback state file is also empty"
            exit 1
        fi
    else
        echo "❌ ERROR: No state files found for workspace ${env.TARGET_WORKSPACE}"
        exit 1
    fi
fi

# Step 2: Configure backend with actual discovered S3 key
echo
echo 'Step 2: Configuring backend with discovered S3 key...'

cat > tofu/aws/modules/airgap/backend.tf << EOF
terraform {
  backend "s3" {
    bucket  = "${env.TF_STATE_BUCKET}"
    key     = "\$ACTUAL_STATE_KEY"
    region  = "${env.TF_STATE_REGION}"
    encrypt = true
    
    # Enhanced backend settings for reliability
    skip_credentials_validation = false
    skip_metadata_api_check     = false
    skip_region_validation      = false
    skip_requesting_account_id  = false
    
    # Note: No DynamoDB locking configured
    # Using direct S3 key access (no workspace needed)
  }
}
EOF

echo "✅ Backend configuration created with discovered key:"
cat tofu/aws/modules/airgap/backend.tf

# Step 3: Initialize OpenTofu with direct S3 key backend
echo
echo 'Step 3: Initializing OpenTofu with direct S3 key access...'

# CRITICAL: Unset TF_WORKSPACE for direct state access
unset TF_WORKSPACE

# Clean any existing backend state
rm -f tofu/aws/modules/airgap/.terraform/terraform.tfstate
rm -rf tofu/aws/modules/airgap/.terraform/terraform.tfstate.d/
rm -f tofu/aws/modules/airgap/.terraform.lock.hcl

# Initialize backend with direct S3 key
if tofu -chdir=tofu/aws/modules/airgap init -input=false -reconfigure; then
    echo "✅ OpenTofu backend initialized successfully with direct S3 key"
else
    echo "❌ ERROR: Failed to initialize OpenTofu backend"
    
    # Debug initialization failure
    echo "Debugging backend initialization failure:"
    echo "1. Checking AWS credentials:"
    aws sts get-caller-identity || echo "AWS credentials check failed"
    
    echo "2. Checking S3 bucket access:"
    aws s3 ls s3://${env.TF_STATE_BUCKET}/ --region ${env.TF_STATE_REGION} || echo "S3 bucket access failed"
    
    echo "3. Checking backend configuration:"
    cat tofu/aws/modules/airgap/backend.tf || echo "Backend config missing"
    
    echo "4. Testing direct state file access:"
    aws s3 ls s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY --region ${env.TF_STATE_REGION} || echo "Direct state file access failed"
    
    exit 1
fi

# Step 4: Verify state resources are loaded (no workspace needed with direct key)
echo
echo 'Step 4: Verifying state resources are loaded...'
LOADED_RESOURCES=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
echo "Resources loaded from state: \$LOADED_RESOURCES"

if [ "\$LOADED_RESOURCES" -eq 0 ]; then
    echo "⚠️ WARNING: No resources found in state"
    echo "This could mean:"
    echo "  1. Infrastructure was already destroyed"
    echo "  2. State file is empty or corrupted"
    echo "  3. Configuration compatibility issues"
    
    echo
    echo "🔍 Additional debugging:"
    echo "Direct state file download test:"
    
    # Try to download and analyze state directly
    TEMP_STATE="/tmp/direct_state_analysis.tfstate"
    if aws s3 cp s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY "\$TEMP_STATE" --region ${env.TF_STATE_REGION} --quiet; then
        echo "✅ State file downloaded successfully"
        
        # Check if state file is empty first
        STATE_FILE_SIZE=\$(stat -c%s "\$TEMP_STATE" 2>/dev/null || echo 0)
        echo "State file size: \$STATE_FILE_SIZE bytes"

        if [ "\$STATE_FILE_SIZE" -eq 0 ]; then
            echo "ℹ️ State file is empty - no resources to destroy"
        elif python3 -c "import json; json.load(open('\$TEMP_STATE'))" 2>/dev/null; then
            DIRECT_RESOURCES=\$(python3 -c "import json; data=json.load(open('\$TEMP_STATE')); print(len(data.get('resources', [])))" 2>/dev/null || echo 0)
            echo "Resources in direct state file: \$DIRECT_RESOURCES"

            if [ "\$DIRECT_RESOURCES" -gt 0 ]; then
                echo "⚠️ State file contains resources but OpenTofu cannot access them"
                echo "This may indicate configuration or provider compatibility issues"
            else
                echo "ℹ️ State file is valid JSON but contains no resources - infrastructure already destroyed"
            fi
        else
            echo "⚠️ State file is not valid JSON, but OpenTofu can still process it"
            echo "This is normal for some Terraform state formats or partially initialized states"
            echo "Since destruction planning works, the state is accessible to OpenTofu"
        fi
        
        rm -f "\$TEMP_STATE"
    else
        echo "❌ Failed to download state file directly"
    fi
    
    if [ "${params.FORCE_DESTROY ?: 'false'}" != "true" ]; then
        echo "❌ No resources to destroy and FORCE_DESTROY not enabled"
        exit 1
    else
        echo "⚠️ FORCE_DESTROY enabled, continuing despite no resources"
    fi
else
    echo "✅ Found \$LOADED_RESOURCES resources ready for destruction"
    echo
    echo "Sample resources in state (first 10):"
    tofu -chdir=tofu/aws/modules/airgap state list | head -10 | while read -r resource; do
        echo "  → \$resource"
    done
fi

echo
echo "✅ SUCCESS: Enhanced state retrieval completed"
echo "   - Backend: S3 Direct Key Access"
echo "   - State file: \$ACTUAL_STATE_KEY"
echo "   - File size: \$STATE_SIZE bytes"
echo "   - Resources available: \$LOADED_RESOURCES"
echo "   - Ready for destruction planning"

# Store the actual state key for use in other functions
echo "\$ACTUAL_STATE_KEY" > /tmp/actual_state_key.txt
"""

        executeScriptInContainer(retrieveScript)
        logInfo('✅ Enhanced state retrieval successful')
        return true
    } catch (Exception e) {
        logError("❌ Enhanced state retrieval failed: ${e.message}")
        
        // Enhanced troubleshooting with fallback options
        logError("💡 Enhanced troubleshooting suggestions:")
        logError("   1. State file found but with non-standard S3 key pattern")
        logError("   2. Run diagnostic script: ./scripts/diagnose_destroy_state_issue.sh ${env.TARGET_WORKSPACE}")
        logError("   3. Use targeted fix script: ./scripts/fix_destroy_state_mismatch.sh ${env.TARGET_WORKSPACE}")
        logError("   4. Verify deployment pipeline completed successfully")
        logError("   5. Check deployment logs for actual workspace name and S3 locations")
        
        return false
    }
}

def executeDestruction() {
    logInfo('Executing direct infrastructure destruction')

    def destructionScript = """
    set -e
    cd /root/go/src/github.com/rancher/qa-infra-automation/

    echo "--- Configuring Backend for Workspace: ${env.TARGET_WORKSPACE} ---"
    # Use a backend.tf file to ensure OpenTofu initializes in s3 mode
    cat <<EOF > tofu/aws/modules/airgap/backend.tf
    terraform {
      backend "s3" {}
    }
EOF

    echo "--- Initializing OpenTofu ---"
    tofu -chdir=tofu/aws/modules/airgap init \
        -backend-config="bucket=${env.TF_STATE_BUCKET}" \
        -backend-config="key=${env.TF_STATE_KEY_PREFIX}/${env.TARGET_WORKSPACE}.tfstate" \
        -backend-config="region=${env.TF_STATE_REGION}"

    echo "--- Selecting Workspace: ${env.TARGET_WORKSPACE} ---"
    tofu -chdir=tofu/aws/modules/airgap workspace select "${env.TARGET_WORKSPACE}"

    echo "--- Starting Direct Destruction ---"
    # Pass placeholder variables directly to the destroy command to satisfy required variables
    tofu -chdir=tofu/aws/modules/airgap destroy -auto-approve \
        -var 'user_id=destroy' \
        -var 'ssh_key=destroy' \
        -var 'ssh_key_name=destroy' \
        -var 'aws_secret_key=destroy' \
        -var 'aws_ami=destroy' \
        -var 'aws_hostname_prefix=destroy' \
        -var 'aws_route53_zone=destroy' \
        -var 'aws_ssh_user=destroy' \
        -var 'aws_security_group=[]' \
        -var 'aws_vpc=destroy' \
        -var 'aws_volume_size=0' \
        -var 'aws_subnet=destroy' \
        -var 'instance_type=destroy'

    echo "--- Verifying Destruction ---"
    REMAINING_RESOURCES=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l || echo 0)
    if [ "\$REMAINING_RESOURCES" -eq 0 ]; then
        echo "✅ Success: All resources have been destroyed."
        # Clean up the workspace
        echo "--- Cleaning up Terraform Workspace ---"
        tofu -chdir=tofu/aws/modules/airgap workspace select default || echo "Could not switch to default workspace."
        tofu -chdir=tofu/aws/modules/airgap workspace delete "${env.TARGET_WORKSPACE}" || echo "Could not delete workspace ${env.TARGET_WORKSPACE}."
    else
        echo "❌ WARNING: \$REMAINING_RESOURCES resources still remain in state."
        tofu -chdir=tofu/aws/modules/airgap state list
        exit 1
    fi
    """

    executeScriptInContainer(destructionScript)
}

def archiveDestructionResults() {
    logInfo('Archiving destruction results')

    try {
        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/destruction-summary.json ./ || true
        """

        logInfo('Destruction results archived')
    } catch (Exception e) {
        logError("Failed to archive destruction results: ${e.message}")
    }
}

def archiveDestructionFailureArtifacts() {
    logInfo('Archiving destruction failure artifacts')

    try {
        def debugCommands = [
            "cd ${env.QA_INFRA_WORK_PATH}",
            'tofu -chdir=tofu/aws/modules/airgap workspace list > workspace-list.txt 2>&1 || echo "No workspace list available"',
            "tofu -chdir=tofu/aws/modules/airgap state list > remaining-resources.txt 2>&1 || echo 'No state available'",
            "echo 'Destruction failure artifact collection completed'"
        ]

        executeInContainer(debugCommands)

        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/workspace-list.txt ./ || true
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/remaining-resources.txt ./ || true
        """

        archiveArtifacts artifacts: 'workspace-list.txt,remaining-resources.txt', allowEmptyArchive: true
    } catch (Exception e) {
        logError("Failed to archive failure artifacts: ${e.message}")
    }
}

def executeInContainer(commands) {
    def commandString = commands.join(' && ')
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"
    def scriptFile = "destroy-commands-${timestamp}.sh"

    writeFile file: scriptFile, text: commandString

    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            -e TF_WORKSPACE=${env.TARGET_WORKSPACE} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def cleanupContainersAndVolumes() {
    logInfo('Cleaning up Docker containers and volumes')

    try {
        sh """
            # Stop and remove containers
            docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker stop || true
            docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker rm -v || true

            # Remove the Docker image
            docker rmi -f ${env.IMAGE_NAME} || true

            # Remove the shared volume
            docker volume rm -f ${env.VALIDATION_VOLUME} || true

            # Clean up dangling resources
            docker system prune -f || true
        """
    } catch (Exception e) {
        logError("Docker cleanup failed: ${e.message}")
    }
}

def archiveBuildArtifacts(artifacts) {
    try {
        archiveArtifacts artifacts: artifacts.join(','), allowEmptyArchive: true
        logInfo("Artifacts archived: ${artifacts.join(', ')}")
    } catch (Exception e) {
        logError("Failed to archive artifacts: ${e.message}")
    }
}

def sendSlackNotification(config) {
    if (env.SLACK_WEBHOOK) {
        try {
            def payload = [
                channel: '#rancher-qa',
                username: 'Jenkins-Destroyer',
                color: config.color,
                title: 'Infrastructure Destruction Pipeline',
                message: config.message,
                fields: [
                    [title: 'Job', value: env.JOB_NAME, short: true],
                    [title: 'Build', value: env.BUILD_NUMBER, short: true],
                    [title: 'Workspace', value: env.TARGET_WORKSPACE, short: true],
                    [title: 'State Source', value: 'S3 Backend', short: true]
                ]
            ]

            httpRequest(
                httpMode: 'POST',
                url: env.SLACK_WEBHOOK,
                contentType: 'APPLICATION_JSON',
                requestBody: groovy.json.JsonOutput.toJson(payload)
            )

            logInfo('Slack notification sent successfully')
        } catch (Exception e) {
            logError("Failed to send Slack notification: ${e.message}")
        }
    }
}

/**
 * LOGGING FUNCTIONS
 */

def logInfo(message) {
    echo "ℹ️ [INFO] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logError(message) {
    echo "❌ [ERROR] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logWarning(message) {
    echo "⚠️ [WARNING] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logDebug(message) {
    if (params.LOG_LEVEL == 'DEBUG' || params.LOG_LEVEL == 'VERBOSE') {
        echo "🔍 [DEBUG] ${new Date().format('HH:mm:ss')} - ${message}"
    }
}
